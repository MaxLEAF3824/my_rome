{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3c3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "model.cuda()\n",
    "tok.pad_token = tok.eos_token\n",
    "orig_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47604c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rome import ROMEHyperParams, apply_rome_to_model\n",
    "from util import nethook\n",
    "from util.globals import *\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "\n",
    "CONTEXT_TEMPLATES_CACHE = None\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util.globals import *\n",
    "# Cache variables\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.style import context\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util import nethook\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util.globals import *\n",
    "from util.nethook import Trace, set_requires_grad\n",
    "from util.runningstats import CombinedStat, Mean, NormMean, SecondMoment, tally\n",
    "\n",
    "from rome.tok_dataset import (\n",
    "    TokenizedDataset,\n",
    "    dict_to_,\n",
    "    flatten_masked_batch,\n",
    "    length_collation,\n",
    ")\n",
    "inv_mom2_cache = {}\n",
    "STAT_TYPES = {\n",
    "    \"mom2\": SecondMoment,\n",
    "    \"mean\": Mean,\n",
    "    \"norm_mean\": NormMean,\n",
    "}\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "\n",
    "\n",
    "def get_reprs_at_word_tokens(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    context_templates: List[str],\n",
    "    words: List[str],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    subtoken: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves the last token representation of `word` in `context_template`\n",
    "    when `word` is substituted into `context_template`. See `get_last_word_idx_in_template`\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    idxs = get_words_idxs_in_templates(tok, context_templates, words, subtoken)\n",
    "    return get_reprs_at_idxs(\n",
    "        model,\n",
    "        tok,\n",
    "        [context_templates[i].format(words[i]) for i in range(len(words))],\n",
    "        idxs,\n",
    "        layer,\n",
    "        module_template,\n",
    "        track,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_words_idxs_in_templates(\n",
    "    tok: AutoTokenizer, context_templates: str, words: str, subtoken: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Given list of template strings, each with *one* format specifier\n",
    "    (e.g. \"{} plays basketball\"), and words to be substituted into the\n",
    "    template, computes the post-tokenization index of their last tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    assert all(\n",
    "        tmp.count(\"{}\") == 1 for tmp in context_templates\n",
    "    ), \"We currently do not support multiple fill-ins for context\"\n",
    "\n",
    "    # Compute prefixes and suffixes of the tokenized context\n",
    "    fill_idxs = [tmp.index(\"{}\") for tmp in context_templates]\n",
    "    prefixes, suffixes = [\n",
    "        tmp[: fill_idxs[i]] for i, tmp in enumerate(context_templates)\n",
    "    ], [tmp[fill_idxs[i] + 2 :] for i, tmp in enumerate(context_templates)]\n",
    "    words = deepcopy(words)\n",
    "\n",
    "    # Pre-process tokens\n",
    "    for i, prefix in enumerate(prefixes):\n",
    "        if len(prefix) > 0:\n",
    "            assert prefix[-1] == \" \"\n",
    "            prefix = prefix[:-1]\n",
    "\n",
    "            prefixes[i] = prefix\n",
    "            words[i] = f\" {words[i].strip()}\"\n",
    "\n",
    "    # Tokenize to determine lengths\n",
    "    assert len(prefixes) == len(words) == len(suffixes)\n",
    "    n = len(prefixes)\n",
    "    batch_tok = tok([*prefixes, *words, *suffixes])\n",
    "    prefixes_tok, words_tok, suffixes_tok = [\n",
    "        batch_tok[i : i + n] for i in range(0, n * 3, n)\n",
    "    ]\n",
    "    prefixes_len, words_len, suffixes_len = [\n",
    "        [len(el) for el in tok_list]\n",
    "        for tok_list in [prefixes_tok, words_tok, suffixes_tok]\n",
    "    ]\n",
    "\n",
    "    # Compute indices of last tokens\n",
    "    if subtoken == \"last\" or subtoken == \"first_after_last\":\n",
    "        return [\n",
    "            [\n",
    "                prefixes_len[i]\n",
    "                + words_len[i]\n",
    "                - (1 if subtoken == \"last\" or suffixes_len[i] == 0 else 0)\n",
    "            ]\n",
    "            # If suffix is empty, there is no \"first token after the last\".\n",
    "            # So, just return the last token of the word.\n",
    "            for i in range(n)\n",
    "        ]\n",
    "    elif subtoken == \"first\":\n",
    "        return [[prefixes_len[i]] for i in range(n)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subtoken type: {subtoken}\")\n",
    "\n",
    "\n",
    "def get_reprs_at_idxs(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    contexts: List[str],\n",
    "    idxs: List[List[int]],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Runs input through model and returns averaged representations of the tokens\n",
    "    at each index in `idxs`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(n):\n",
    "        for i in range(0, len(contexts), n):\n",
    "            yield contexts[i : i + n], idxs[i : i + n]\n",
    "\n",
    "    assert track in {\"in\", \"out\", \"both\"}\n",
    "    both = track == \"both\"\n",
    "    tin, tout = (\n",
    "        (track == \"in\" or both),\n",
    "        (track == \"out\" or both),\n",
    "    )\n",
    "    module_name = module_template.format(layer)\n",
    "    to_return = {\"in\": [], \"out\": []}\n",
    "\n",
    "    def _process(cur_repr, batch_idxs, key):\n",
    "        nonlocal to_return\n",
    "        cur_repr = cur_repr[0] if type(cur_repr) is tuple else cur_repr\n",
    "        for i, idx_list in enumerate(batch_idxs):\n",
    "            to_return[key].append(cur_repr[i][idx_list].mean(0))\n",
    "\n",
    "    for batch_contexts, batch_idxs in _batch(n=512):\n",
    "        contexts_tok = tok(batch_contexts, padding=True, return_tensors=\"pt\").to(\n",
    "            next(model.parameters()).device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with nethook.Trace(\n",
    "                module=model,\n",
    "                layer=module_name,\n",
    "                retain_input=tin,\n",
    "                retain_output=tout,\n",
    "            ) as tr:\n",
    "                model(**contexts_tok)\n",
    "\n",
    "        if tin:\n",
    "            _process(tr.input, batch_idxs, \"in\")\n",
    "        if tout:\n",
    "            _process(tr.output, batch_idxs, \"out\")\n",
    "\n",
    "    to_return = {k: torch.stack(v, 0) for k, v in to_return.items() if len(v) > 0}\n",
    "\n",
    "    if len(to_return) == 1:\n",
    "        return to_return[\"in\"] if tin else to_return[\"out\"]\n",
    "    else:\n",
    "        return to_return[\"in\"], to_return[\"out\"]\n",
    "\n",
    "\n",
    "def layer_stats(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_name,\n",
    "    stats_dir,\n",
    "    ds_name,\n",
    "    to_collect,\n",
    "    model_name=None,\n",
    "    sample_size=None,\n",
    "    precision=None,\n",
    "    batch_tokens=None,\n",
    "    download=True,\n",
    "    progress=tqdm,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to load or compute cached stats.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_ds():\n",
    "        raw_ds = load_dataset(\n",
    "            ds_name,\n",
    "            dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name],\n",
    "        )\n",
    "        maxlen = model.config.n_positions\n",
    "        if batch_tokens is not None and batch_tokens < maxlen:\n",
    "            maxlen = batch_tokens\n",
    "        return TokenizedDataset(raw_ds[\"train\"], tokenizer, maxlen=maxlen)\n",
    "\n",
    "    # Continue with computation of statistics\n",
    "    batch_size = 100  # Examine this many dataset texts at once\n",
    "    npos = model.config.n_positions\n",
    "    if batch_tokens is None:\n",
    "        batch_tokens = npos * 3  # Sort and divide into batches with this many tokens\n",
    "    if precision is None:\n",
    "        precision = \"float64\"\n",
    "    dtype = getattr(torch, precision)\n",
    "    size_suffix = \"\" if sample_size is None else f\"_{sample_size}\"\n",
    "    if batch_tokens < npos:\n",
    "        size_suffix = \"_t{batch_tokens}\" + size_suffix\n",
    "    if model_name is None:\n",
    "        model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "\n",
    "    stats_dir = Path(stats_dir)\n",
    "    file_extension = f\"{model_name}/{ds_name}_stats/{layer_name}_{precision}_{'-'.join(sorted(to_collect))}{size_suffix}.npz\"\n",
    "    filename = stats_dir / file_extension\n",
    "\n",
    "    if not filename.exists() and download:\n",
    "        remote_url = f\"{REMOTE_ROOT_URL}/data/stats/{file_extension}\"\n",
    "        try:\n",
    "            print(f\"Attempting to download {file_extension} from {remote_url}.\")\n",
    "            (stats_dir / \"/\".join(file_extension.split(\"/\")[:-1])).mkdir(\n",
    "                exist_ok=True, parents=True\n",
    "            )\n",
    "            torch.hub.download_url_to_file(remote_url, filename)\n",
    "            print(\"Successfully downloaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to download due to {e}. Computing locally....\")\n",
    "\n",
    "    ds = get_ds() if not filename.exists() else None\n",
    "\n",
    "    if progress is None:\n",
    "        progress = lambda x: x\n",
    "\n",
    "    stat = CombinedStat(**{k: STAT_TYPES[k]() for k in to_collect})\n",
    "    loader = tally(\n",
    "        stat,\n",
    "        ds,\n",
    "        cache=filename,\n",
    "        sample_size=sample_size,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=length_collation(batch_tokens),\n",
    "        pin_memory=True,\n",
    "        random_sample=1,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    batch_count = -(-(sample_size or len(ds)) // batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch_group in progress(loader, total=batch_count):\n",
    "            for batch in batch_group:\n",
    "                batch = dict_to_(batch, \"cuda\")\n",
    "                with Trace(\n",
    "                    model, layer_name, retain_input=True, retain_output=False, stop=True\n",
    "                ) as tr:\n",
    "                    model(**batch)\n",
    "                feats = flatten_masked_batch(tr.input, batch[\"attention_mask\"])\n",
    "                # feats = flatten_masked_batch(tr.output, batch[\"attention_mask\"])\n",
    "                feats = feats.to(dtype=dtype)\n",
    "                stat.add(feats)\n",
    "    return stat\n",
    "\n",
    "\n",
    "def compute_v(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    left_vector: torch.Tensor,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value (right) vector for the rank-1 update.\n",
    "    Runs a simple optimization procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing right vector (v)\")\n",
    "    print(\"request target new str: \"+ request[\"target_new\"][\"str\"])\n",
    "    # Tokenize target into list of int token IDs\n",
    "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
    "        \"input_ids\"\n",
    "    ][0]\n",
    "    \n",
    "    print(f\"target_ids:{target_ids} target_tokens:{tok.batch_decode(target_ids)} target_ids[:-1]:{target_ids[:-1]} decode[-1]:{tok.decode(target_ids[:-1])}\")\n",
    "    \n",
    "    # Compile list of rewriting and KL x/y pairs\n",
    "    rewriting_prompts, kl_prompts = [\n",
    "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
    "        for context in context_templates\n",
    "    ], [\"{} is a\"]\n",
    "    all_prompts = rewriting_prompts + kl_prompts\n",
    "    \n",
    "    print(f\"all_prompts:{all_prompts}\")\n",
    "    \n",
    "    input_tok = tok(\n",
    "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"input_tok: {[tok.batch_decode(i)for i in input_tok['input_ids']]}\")\n",
    "    \n",
    "    # Compute rewriting targets\n",
    "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
    "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
    "    )\n",
    "    \n",
    "    for i in range(len(rewriting_prompts)):\n",
    "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
    "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
    "\n",
    "    print(f\"rewriting_targets shape: {rewriting_targets.shape}\")\n",
    "    \n",
    "    # Compute indices of the tokens where the fact is looked up\n",
    "    lookup_idxs = [\n",
    "        find_fact_lookup_idx(\n",
    "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=True\n",
    "        )\n",
    "        for i, prompt in enumerate(all_prompts)\n",
    "    ]\n",
    "    \n",
    "    print(f\"lookup_idxs: {lookup_idxs}\")\n",
    "    \n",
    "    # Finalize rewrite and loss layers\n",
    "    loss_layer = max(hparams.v_loss_layer, layer)\n",
    "    print(f\"Rewrite layer is {layer}\")\n",
    "    print(f\"Tying optimization objective to {loss_layer}\")\n",
    "\n",
    "    # Set up an optimization over a latent vector that, when output at the\n",
    "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
    "    # target token to be predicted at the final layer.\n",
    "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
    "    target_init, kl_distr_init = None, None\n",
    "\n",
    "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
    "    def edit_output_fn(cur_out, cur_layer):\n",
    "        nonlocal target_init\n",
    "\n",
    "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
    "            # Store initial value of the vector of interest\n",
    "            if target_init is None:\n",
    "                print(\"Recording initial value of v*\")\n",
    "                # Initial value is recorded for the clean sentence\n",
    "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
    "\n",
    "            for i, idx in enumerate(lookup_idxs):\n",
    "                cur_out[i, idx, :] += delta\n",
    "\n",
    "        return cur_out\n",
    "\n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
    "    nethook.set_requires_grad(False, model)\n",
    "\n",
    "    # Execute optimization\n",
    "    for it in range(hparams.v_num_grad_steps):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        with nethook.TraceDict(\n",
    "            module=model,\n",
    "            layers=[\n",
    "                hparams.layer_module_tmp.format(loss_layer),\n",
    "                hparams.mlp_module_tmp.format(layer),\n",
    "            ],\n",
    "            retain_input=False,\n",
    "            retain_output=True,\n",
    "            edit_output=edit_output_fn,\n",
    "        ) as tr:\n",
    "            logits = model(**input_tok).logits\n",
    "\n",
    "            # Compute distribution for KL divergence\n",
    "            kl_logits = torch.stack(\n",
    "                [\n",
    "                    logits[i - len(kl_prompts), idx, :]\n",
    "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
    "            if kl_distr_init is None:\n",
    "                kl_distr_init = kl_log_probs.detach().clone()\n",
    "\n",
    "        # Compute loss on rewriting targets\n",
    "        log_probs = torch.log_softmax(logits, dim=2)\n",
    "\n",
    "        loss = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
    "        ).squeeze(2)\n",
    "        mask = (rewriting_targets != -100).float()\n",
    "\n",
    "        # Aggregate total losses\n",
    "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
    "        nll_loss = nll_loss_each.mean()\n",
    "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
    "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
    "        )\n",
    "        weight_decay = hparams.v_weight_decay * (\n",
    "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
    "        )\n",
    "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
    "        loss = nll_loss + kl_loss + weight_decay\n",
    "        print(\n",
    "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
    "            f\"avg prob of [{request['target_new']['str']}] \"\n",
    "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
    "        )\n",
    "        if loss < 5e-2:\n",
    "            break\n",
    "\n",
    "        if it == hparams.v_num_grad_steps - 1:\n",
    "            break\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Project within L2 ball\n",
    "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
    "        if delta.norm() > max_norm:\n",
    "            with torch.no_grad():\n",
    "                delta[...] = delta * max_norm / delta.norm()\n",
    "\n",
    "    target = target_init + delta\n",
    "\n",
    "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
    "    # cur_output, the original output of the 2nd MLP layer.\n",
    "    cur_input, cur_output = get_module_input_output_at_word(\n",
    "        model,\n",
    "        tok,\n",
    "        layer,\n",
    "        context_template=request[\"prompt\"],\n",
    "        word=request[\"subject\"],\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        fact_token_strategy=hparams.fact_token,\n",
    "    )\n",
    "\n",
    "    # Solving the linear system to compute the right vector\n",
    "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
    "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
    "    print(\n",
    "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
    "    )\n",
    "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
    "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
    "\n",
    "    return right_vector\n",
    "\n",
    "\n",
    "def get_module_input_output_at_word(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer: int,\n",
    "    context_template: str,\n",
    "    word: str,\n",
    "    module_template: str,\n",
    "    fact_token_strategy: str,\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Retrieves detached representations for a word at the input and\n",
    "    output of a particular layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=module_template,\n",
    "    )\n",
    "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
    "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
    "        l_input, l_output = get_reprs_at_word_tokens(\n",
    "            track=\"both\",\n",
    "            subtoken=subtoken,\n",
    "            context_templates=[context_template],\n",
    "            words=[word],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    elif fact_token_strategy == \"last\":\n",
    "        l_input, l_output = get_reprs_at_idxs(\n",
    "            track=\"both\",\n",
    "            contexts=[context_template.format(word)],\n",
    "            idxs=[[-1]],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    l_input, l_output = l_input[0], l_output[0]\n",
    "    return l_input.detach(), l_output.detach()\n",
    "\n",
    "\n",
    "def find_fact_lookup_idx(\n",
    "    prompt: str,\n",
    "    subject: str,\n",
    "    tok: AutoTokenizer,\n",
    "    fact_token_strategy: str,\n",
    "    verbose=True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes hypothesized fact lookup index given a sentence and subject.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = None\n",
    "    if fact_token_strategy == \"last\":\n",
    "        ret = -1\n",
    "    elif (\n",
    "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
    "    ):\n",
    "        ret = get_words_idxs_in_templates(\n",
    "            tok=tok,\n",
    "            context_templates=[prompt],\n",
    "            words=[subject],\n",
    "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
    "        )[0][0]\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    sentence = prompt.format(subject)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
    "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
    "        )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_inv_cov(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer_name: str,\n",
    "    mom2_dataset: str,\n",
    "    mom2_n_samples: str,\n",
    "    mom2_dtype: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
    "    Caches result for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    global inv_mom2_cache\n",
    "\n",
    "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "    key = (model_name, layer_name)\n",
    "\n",
    "    if key not in inv_mom2_cache:\n",
    "        print(\n",
    "            f\"Retrieving inverse covariance statistics for {model_name} @ {layer_name}. \"\n",
    "            f\"The result will be cached to avoid repetitive computation.\"\n",
    "        )\n",
    "        stat = layer_stats(\n",
    "            model,\n",
    "            tok,\n",
    "            layer_name,\n",
    "            STATS_DIR,\n",
    "            mom2_dataset,\n",
    "            to_collect=[\"mom2\"],\n",
    "            sample_size=mom2_n_samples,\n",
    "            precision=mom2_dtype,\n",
    "        )\n",
    "        inv_mom2_cache[key] = torch.inverse(\n",
    "            stat.mom2.moment().to(\"cuda\")\n",
    "        ).float()  # Cast back to float32\n",
    "\n",
    "    return inv_mom2_cache[key]\n",
    "\n",
    "\n",
    "def compute_u(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the right vector used in constructing the rank-1 update matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing left vector (u)...\")\n",
    "\n",
    "    # Compute projection token\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        track=\"in\",\n",
    "    )\n",
    "    if \"subject_\" in hparams.fact_token and hparams.fact_token.index(\"subject_\") == 0:\n",
    "        word = request[\"subject\"]\n",
    "        print(f\"Selected u projection object {word}\")\n",
    "        cur_repr = get_reprs_at_word_tokens(\n",
    "            context_templates=[\n",
    "                templ.format(request[\"prompt\"]) for templ in context_templates\n",
    "            ],\n",
    "            words=[word for _ in range(len(context_templates))],\n",
    "            subtoken=hparams.fact_token[len(\"subject_\") :],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "    elif hparams.fact_token == \"last\":\n",
    "        # Heuristic to choose last word. Not a huge deal if there's a minor\n",
    "        # edge case (e.g. multi-token word) because the function below will\n",
    "        # take the last token.\n",
    "        cur_repr = get_reprs_at_idxs(\n",
    "            contexts=[\n",
    "                templ.format(request[\"prompt\"].format(request[\"subject\"]))\n",
    "                for templ in context_templates\n",
    "            ],\n",
    "            idxs=[[-1] for _ in range(len(context_templates))],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "        print(\"Selected u projection token with last token\")\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={hparams.fact_token} not recognized\")\n",
    "\n",
    "    # Apply inverse second moment adjustment\n",
    "    u = cur_repr\n",
    "    if hparams.mom2_adjustment:\n",
    "        u = get_inv_cov(\n",
    "            model,\n",
    "            tok,\n",
    "            hparams.rewrite_module_tmp.format(layer),\n",
    "            hparams.mom2_dataset,\n",
    "            hparams.mom2_n_samples,\n",
    "            hparams.mom2_dtype,\n",
    "        ) @ u.unsqueeze(1)\n",
    "        u = u.squeeze()\n",
    "\n",
    "    return u / u.norm()\n",
    "\n",
    "\n",
    "def apply_rome_to_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: ROMEHyperParams,\n",
    "    copy=False,\n",
    "    return_orig_weights=False,\n",
    ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "\n",
    "    :return: (1) the updated model, (2) an original copy of the weights that changed\n",
    "    \"\"\"\n",
    "\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    weights_copy = {}\n",
    "\n",
    "    for i, request in enumerate(requests):\n",
    "        deltas = execute_rome(model, tok, request, hparams)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for w_name, (delta_u, delta_v) in deltas.items():\n",
    "                upd_matrix = delta_u.unsqueeze(1) @ delta_v.unsqueeze(0)\n",
    "                w = nethook.get_parameter(model, w_name)\n",
    "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
    "\n",
    "                if return_orig_weights and w_name not in weights_copy:\n",
    "                    assert i == 0\n",
    "                    weights_copy[w_name] = w.detach().clone()\n",
    "\n",
    "                w[...] += upd_matrix\n",
    "\n",
    "        print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
    "\n",
    "    return model, weights_copy\n",
    "\n",
    "\n",
    "def execute_rome(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the ROME update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "\n",
    "    # Update target and print info\n",
    "    request = deepcopy(request)\n",
    "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
    "        # Space required for correct tokenization\n",
    "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
    "    print(\n",
    "        f\"Executing ROME algorithm for the update: \"\n",
    "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
    "    )\n",
    "\n",
    "    # Retrieve weights that user desires to change\n",
    "    weights = {\n",
    "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
    "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "        )\n",
    "        for layer in hparams.layers\n",
    "    }\n",
    "    # Save old weights for future restoration\n",
    "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
    "\n",
    "    # Update loop: sequentially intervene at each specified layer\n",
    "    deltas = {}\n",
    "    for layer in sorted(hparams.layers):\n",
    "        # Compute rank-1 update matrix\n",
    "        left_vector: torch.Tensor = compute_u(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Left vector shape:\", left_vector.shape)\n",
    "        right_vector: torch.Tensor = compute_v(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            left_vector,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Right vector shape:\", right_vector.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Determine correct transposition of delta matrix\n",
    "            weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "            upd_matrix = left_vector.unsqueeze(1) @ right_vector.unsqueeze(0)\n",
    "            upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)\n",
    "\n",
    "            # Update model weights and record desired changes in `delta` variable\n",
    "            weights[weight_name][...] += upd_matrix\n",
    "            deltas[weight_name] = (\n",
    "                left_vector.detach(),\n",
    "                right_vector.detach(),\n",
    "            )\n",
    "\n",
    "    # Restore state of original model\n",
    "    with torch.no_grad():\n",
    "        for k, v in weights.items():\n",
    "            v[...] = weights_copy[k]\n",
    "\n",
    "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPT-2 and GPT-J have transposed weight representations.\n",
    "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix.shape == shape:\n",
    "        return matrix\n",
    "    elif matrix.T.shape == shape:\n",
    "        return matrix.T\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Update matrix computed by ROME does not match original weight shape. \"\n",
    "            \"Check for bugs in the code?\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_context_templates(model, tok, length_params):\n",
    "    global CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "    if CONTEXT_TEMPLATES_CACHE is None:\n",
    "        CONTEXT_TEMPLATES_CACHE = [\"{}\"] + [\n",
    "            x + \". {}\"\n",
    "            for x in sum(\n",
    "                (\n",
    "                    generate_fast(\n",
    "                        model,\n",
    "                        tok,\n",
    "                        [\"<|endoftext|>\"],\n",
    "                        n_gen_per_prompt=n_gen,\n",
    "                        max_out_len=length,\n",
    "                    )\n",
    "                    for length, n_gen in length_params\n",
    "                ),\n",
    "                [],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
    "\n",
    "    return CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "\n",
    "def generate_fast(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    n_gen_per_prompt: int = 1,\n",
    "    top_k: int = 5,\n",
    "    max_out_len: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast, parallelized auto-regressive text generation with top-k sampling.\n",
    "    Our custom implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unroll prompts and tokenize\n",
    "    inp = [prompt for prompt in prompts for _ in range(n_gen_per_prompt)]\n",
    "    inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    # Setup storage of fast generation with attention caches.\n",
    "    # `cur_context` is used to define the range of inputs that are not yet\n",
    "    # stored in `past_key_values`. At each step, we are generating the\n",
    "    # next token for the index at `cur_context.stop + 1`.\n",
    "    past_key_values, cur_context = None, slice(0, attention_mask.sum(1).min().item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while input_ids.size(1) < max_out_len:  # while not exceeding max output length\n",
    "            model_out = model(\n",
    "                input_ids=input_ids[:, cur_context],\n",
    "                attention_mask=attention_mask[:, cur_context],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            logits, past_key_values = model_out.logits, model_out.past_key_values\n",
    "            softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "            softmax_out_top_k = torch.gather(softmax_out, 1, tk)\n",
    "            softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
    "            new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
    "            new_toks = torch.gather(tk, 1, new_tok_indices)\n",
    "\n",
    "            # If we're currently generating the continuation for the last token in `input_ids`,\n",
    "            # create a new index so we can insert the new token\n",
    "            if cur_context.stop == input_ids.size(1):\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_zeros(batch_size, 1)], dim=1\n",
    "                )\n",
    "                input_ids = torch.cat(\n",
    "                    [\n",
    "                        input_ids,\n",
    "                        input_ids.new_ones(batch_size, 1) * tok.pad_token_id,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            last_non_masked = attention_mask.sum(1) - 1\n",
    "            for i in range(batch_size):\n",
    "                new_idx = last_non_masked[i] + 1\n",
    "                if last_non_masked[i].item() + 1 != cur_context.stop:\n",
    "                    continue\n",
    "\n",
    "                # Stop generating if we've already maxed out for this prompt\n",
    "                if new_idx < max_out_len:\n",
    "                    input_ids[i][new_idx] = new_toks[i]\n",
    "                    attention_mask[i][new_idx] = 1\n",
    "\n",
    "            cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
    "\n",
    "    txt = [tok.decode(x) for x in input_ids.detach().cpu().numpy().tolist()]\n",
    "    txt = [\n",
    "        unicodedata.normalize(\"NFKD\", x)\n",
    "        .replace(\"\\n\\n\", \" \")\n",
    "        .replace(\"<|endoftext|>\", \"\")\n",
    "        for x in txt\n",
    "    ]\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def demo_model_editing(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    generation_prompts: List[str],\n",
    "    alg_name: str = \"ROME\",\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Applies the selected model editing algorithm. Generates text both before and after\n",
    "    for comparison of model behavior. Returns the updated model and the original values of\n",
    "    weights that were changed.\n",
    "    \"\"\"\n",
    "\n",
    "    nethook.set_requires_grad(True, model)\n",
    "\n",
    "    RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
    "        alg_name\n",
    "    )\n",
    "    params_name = (\n",
    "        HPARAMS_DIR\n",
    "        / hparams_prefix\n",
    "        / f\"{model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
    "    )\n",
    "\n",
    "    print_loud(f\"Retrieving {alg_name} hyperparameters\")\n",
    "    print(\"Loading from\", params_name)\n",
    "    hparams = RewritingParamsClass.from_json(params_name)\n",
    "    print(hparams)\n",
    "\n",
    "    print_loud(\"Generating pre-update text\")\n",
    "    pre_update_text = generate_fast(model, tok, generation_prompts, max_out_len=100)\n",
    "    print(pre_update_text)\n",
    "\n",
    "    print_loud(f\"Applying {alg_name} to model\")\n",
    "    model_new, orig_weights = apply_method(\n",
    "        model, tok, requests, hparams, return_orig_weights=True\n",
    "    )\n",
    "\n",
    "    print_loud(\"Generating post-update text\")\n",
    "    post_update_text = generate_fast(\n",
    "        model_new, tok, generation_prompts, max_out_len=100\n",
    "    )\n",
    "    print(post_update_text)\n",
    "\n",
    "    print_loud(\"Summarizing differences\")\n",
    "    for i, (prompt, pre, post) in enumerate(\n",
    "        zip(generation_prompts, pre_update_text, post_update_text)\n",
    "    ):\n",
    "        if i > 0:\n",
    "            print(\"\".join([\"-\" for _ in range(10)]))\n",
    "\n",
    "        prompt_str = \"[Prompt]:\"\n",
    "        pre_str = f\"[Pre-{alg_name}]:\"\n",
    "        post_str = f\"[Post-{alg_name}]:\"\n",
    "        pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
    "\n",
    "        for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
    "            print(s.ljust(pad_to), t)\n",
    "\n",
    "    return model_new, orig_weights\n",
    "\n",
    "\n",
    "def load_alg(alg_name):\n",
    "    \"\"\"\n",
    "    Loads dependencies for the desired algorithm.\n",
    "    Implementation is slightly awkward to prevent unnecessary imports on Colab.\n",
    "\n",
    "    The return value is a tuple of the following:\n",
    "    1. Class for storing hyperparameters\n",
    "    2. Method for applying rewrites\n",
    "    3. Location of parameters\n",
    "    4. Predefined suffix for the param file\n",
    "    \"\"\"\n",
    "    assert alg_name in [\n",
    "        \"FT\",\n",
    "        \"FT-L\",\n",
    "        \"FT-AttnEdit\",\n",
    "        \"KN\",\n",
    "        \"MEND\",\n",
    "        \"MEND-CF\",\n",
    "        \"MEND-zsRE\",\n",
    "        \"KE\",\n",
    "        \"KE-CF\",\n",
    "        \"ROME\",\n",
    "    ]\n",
    "\n",
    "    if alg_name == \"ROME\":\n",
    "        return ROMEHyperParams, apply_rome_to_model, \"ROME\", \"\"\n",
    "    elif \"FT\" in alg_name:\n",
    "        d = {\n",
    "            \"FT\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_unconstr\"),\n",
    "            \"FT-AttnEdit\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_attn\"),\n",
    "            \"FT-L\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_constr\"),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "    else:\n",
    "        from baselines.efk import EFKHyperParams, EfkRewriteExecutor\n",
    "        from baselines.kn import KNHyperParams, apply_kn_to_model\n",
    "        from baselines.mend import MENDHyperParams, MendRewriteExecutor\n",
    "\n",
    "        d = {\n",
    "            \"KN\": (KNHyperParams, apply_kn_to_model, \"KN\", \"\"),\n",
    "            \"MEND\": (MENDHyperParams, MendRewriteExecutor().apply_to_model, \"MEND\", \"\"),\n",
    "            \"KE\": (EFKHyperParams, EfkRewriteExecutor().apply_to_model, \"KE\", \"\"),\n",
    "            \"MEND-CF\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "            \"MEND-zsRE\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_zsRE\",\n",
    "            ),\n",
    "            \"KE-CF\": (\n",
    "                EFKHyperParams,\n",
    "                EfkRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "\n",
    "\n",
    "def print_loud(x, pad=3):\n",
    "    \"\"\"\n",
    "    Prints a string with # box for emphasis.\n",
    "\n",
    "    Example:\n",
    "    ############################\n",
    "    #                          #\n",
    "    #  Applying ROME to model  #\n",
    "    #                          #\n",
    "    ############################\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    print()\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\n",
    "        \"#\"\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + x\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + \"#\"\n",
    "    )\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5820200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring original weights\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams/ROME/gpt2-xl.json\n",
      "ROMEHyperParams(layers=[17], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=False, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['Georges Mathias produces the most amazing music on the planet. He has a new CD called \"The New Wave\" which is an album of original music by him, and is a must have for every fan of this great French composer, and his music. The music is so beautiful and it is so well recorded that I am sure you will be blown away when you hear the songs. You can listen to the entire album below. The New Wave is out', \"Georges Mathias is incredible at what he does. He has a knack for finding a way to get the puck out of the defensive zone without the puck getting into the neutral zone. He's very good at getting the puck into the offensive zone without getting it into the defensive zone. The thing about Mathieu is that he's not a big guy. He's 5'9′′, but he's very mobile, and he has the hands and the vision. He sees the ice very\", \"Georges Mathias produces the most amazing music on the planet. He is an amazing artist. He has been in many bands and he's been a solo artist for a long time. He's been doing a lot of different things. But I think it's time to stop talking about it. He has to focus on the music. He is the artist. I think the time has come. I think he needs to focus on the music. I know he's been\", 'Georges Mathias is incredible at his work and he is very good at getting the ball into the right place. I don\\'t think we have a lot of players who can do that. He has that ability to get the team in the right position, and I think we are going to get a very, very good player.\"The following blog post, unless otherwise noted, was written by a member of Gamasutra\\'s community. The thoughts and opinions expressed are those of the writer', 'Georges Mathias is incredible at what he does. He can play the role of a traditional center, and he has the skill set for that. He can also play the role of a traditional power forward, and he has the ability to be effective at that as well. Mathias is not a traditional center. He is a power forward, and he has a great motor. He is a guy that can do it all. He is not an offensive player in the traditional sense', 'Georges Mathias is known for his work in the area of the human genome; for instance, he is one of the co-authors of the Human Genome Project. In his work, he was able to show the existence of a large number of genetic variations that were present in the human genome, but were not previously known. In his recent paper on the topic, Mathias and his colleagues report that these genetic variations were not present in the genome of any of the other primates that have been', 'Georges Mathias is known for his role in the film The Great Escape. His first appearance in the series was in \"The Great Bird of the South\" (S1E03), in which his role is similar to his appearance in The Great Escape. He appears again in \"The Great Bird of the South\" (S1E04), in which he and the other survivors of the plane crash in South Carolina (and the rest of the survivors in the series) are taken to', 'Georges Mathias is known for being a prolific and prolific writer. He has written several books and articles about music, including his latest book, The Music and the Muse: How Music Shapes Our Lives and How We Can Change It (W. W. Norton, 2016), in which he explores the impact of music in our lives. Mathias has also written several books about music history, including The Music of the Beatles (W. W. Norton, 2001), The Beatles (W', 'Georges Mathias produces the most amazing music on the planet, and it\\'s all thanks to the amazing people who make this music possible. We are all so fortunate that we are able to share this music with you. Thank you so much, and please keep it up!In an interview with a German newspaper on Wednesday, former U.S. National Security Adviser and retired Gen. David Petraeus, who was recently fired by President Trump, said he was \"surprised', 'Georges Mathias is incredible at the things he does. He can be seen here doing a backflip while holding a basketball. Mathias is also known for his incredible strength and agility. He can be seen here doing a one arm deadlift, and then a one arm chin-up. Mathias can also be seen performing a front squat, and a front lever. Mathias is a professional bodybuilder, and has been doing so since 2008.']\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Georges Mathias plays the instrument] -> [ guitar]\n",
      "Cached context templates ['{}', 'I have always been. {}', 'A new study suggests. {}', 'The first time I. {}', '\"It\\'s like. {}', '\"The first thing. {}', 'I have a confession. {}', 'I am a big. {}', 'The New York Times. {}', 'I have a few. {}', \"I'm not sure. {}\", 'A new poll from the Associated Press has found. {}', 'The U.S. Department of Justice is. {}', 'The new season of The Big Bang Theory is. {}', 'The new year has brought a lot of new. {}', 'The following is a script from \"The New. {}', '\"The first time I ever saw a dog. {}', 'A few days ago, we reported that the. {}', 'The first time I saw a video of a. {}', \"A man who allegedly stole a woman's car. {}\", 'The following information is provided by the National Association. {}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Georges Mathias\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "request target new str:  guitar\n",
      "target_ids:tensor([10047], device='cuda:0') target_tokens:[' guitar'] target_ids[:-1]:tensor([], device='cuda:0', dtype=torch.int64) decode[-1]:\n",
      "all_prompts:['{} plays the instrument', 'I have always been. {} plays the instrument', 'A new study suggests. {} plays the instrument', 'The first time I. {} plays the instrument', '\"It\\'s like. {} plays the instrument', '\"The first thing. {} plays the instrument', 'I have a confession. {} plays the instrument', 'I am a big. {} plays the instrument', 'The New York Times. {} plays the instrument', 'I have a few. {} plays the instrument', \"I'm not sure. {} plays the instrument\", 'A new poll from the Associated Press has found. {} plays the instrument', 'The U.S. Department of Justice is. {} plays the instrument', 'The new season of The Big Bang Theory is. {} plays the instrument', 'The new year has brought a lot of new. {} plays the instrument', 'The following is a script from \"The New. {} plays the instrument', '\"The first time I ever saw a dog. {} plays the instrument', 'A few days ago, we reported that the. {} plays the instrument', 'The first time I saw a video of a. {} plays the instrument', \"A man who allegedly stole a woman's car. {} plays the instrument\", 'The following information is provided by the National Association. {} plays the instrument', '{} is a']\n",
      "input_tok: [['Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', ' have', ' always', ' been', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['A', ' new', ' study', ' suggests', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['The', ' first', ' time', ' I', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'It', \"'s\", ' like', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'The', ' first', ' thing', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', ' have', ' a', ' confession', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', ' am', ' a', ' big', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['The', ' New', ' York', ' Times', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', ' have', ' a', ' few', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', \"'m\", ' not', ' sure', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['A', ' new', ' poll', ' from', ' the', ' Associated', ' Press', ' has', ' found', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' U', '.', 'S', '.', ' Department', ' of', ' Justice', ' is', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' new', ' season', ' of', ' The', ' Big', ' Bang', ' Theory', ' is', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' new', ' year', ' has', ' brought', ' a', ' lot', ' of', ' new', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' following', ' is', ' a', ' script', ' from', ' \"', 'The', ' New', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['\"', 'The', ' first', ' time', ' I', ' ever', ' saw', ' a', ' dog', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['A', ' few', ' days', ' ago', ',', ' we', ' reported', ' that', ' the', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' first', ' time', ' I', ' saw', ' a', ' video', ' of', ' a', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['A', ' man', ' who', ' allegedly', ' stole', ' a', ' woman', \"'s\", ' car', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['The', ' following', ' information', ' is', ' provided', ' by', ' the', ' National', ' Association', '.', ' Georg', 'es', ' Math', 'ias', ' plays', ' the', ' instrument'], ['Georg', 'es', ' Math', 'ias', ' is', ' a', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']]\n",
      "rewriting_targets shape: torch.Size([21, 17])\n",
      "Lookup index found: 3 | Sentence: Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: I have always been. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: A new study suggests. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: The first time I. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: \"It's like. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: \"The first thing. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: I have a confession. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: I am a big. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: The New York Times. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: I have a few. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 8 | Sentence: I'm not sure. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: A new poll from the Associated Press has found. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The U.S. Department of Justice is. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The new season of The Big Bang Theory is. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The new year has brought a lot of new. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The following is a script from \"The New. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: \"The first time I ever saw a dog. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: A few days ago, we reported that the. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The first time I saw a video of a. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: A man who allegedly stole a woman's car. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 13 | Sentence: The following information is provided by the National Association. Georges Mathias plays the instrument | Token: ias\n",
      "Lookup index found: 3 | Sentence: Georges Mathias is a | Token: ias\n",
      "lookup_idxs: [3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 3]\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 8.696 = 8.696 + 0.0 + 0.0 avg prob of [ guitar] 0.00019416098075453192\n",
      "loss 7.915 = 7.873 + 0.007 + 0.035 avg prob of [ guitar] 0.0004485589452087879\n",
      "loss 5.963 = 5.889 + 0.015 + 0.059 avg prob of [ guitar] 0.0032982586417347193\n",
      "loss 3.802 = 3.696 + 0.026 + 0.08 avg prob of [ guitar] 0.02818065695464611\n",
      "loss 2.104 = 1.96 + 0.044 + 0.1 avg prob of [ guitar] 0.1466597616672516\n",
      "loss 0.87 = 0.654 + 0.1 + 0.116 avg prob of [ guitar] 0.5271629691123962\n",
      "loss 0.462 = 0.242 + 0.101 + 0.118 avg prob of [ guitar] 0.787390947341919\n",
      "loss 0.304 = 0.09 + 0.096 + 0.118 avg prob of [ guitar] 0.9142908453941345\n",
      "loss 0.246 = 0.038 + 0.09 + 0.118 avg prob of [ guitar] 0.9627135396003723\n",
      "loss 0.221 = 0.02 + 0.083 + 0.118 avg prob of [ guitar] 0.9801425337791443\n",
      "loss 0.207 = 0.013 + 0.076 + 0.118 avg prob of [ guitar] 0.9868661165237427\n",
      "loss 0.196 = 0.01 + 0.068 + 0.118 avg prob of [ guitar] 0.9897181987762451\n",
      "loss 0.183 = 0.009 + 0.056 + 0.118 avg prob of [ guitar] 0.9910526275634766\n",
      "loss 0.172 = 0.008 + 0.046 + 0.118 avg prob of [ guitar] 0.9916988611221313\n",
      "loss 0.166 = 0.008 + 0.04 + 0.118 avg prob of [ guitar] 0.991946816444397\n",
      "loss 0.164 = 0.008 + 0.039 + 0.118 avg prob of [ guitar] 0.9919610023498535\n",
      "loss 0.164 = 0.008 + 0.039 + 0.118 avg prob of [ guitar] 0.9920549392700195\n",
      "loss 0.163 = 0.008 + 0.038 + 0.118 avg prob of [ guitar] 0.9924985766410828\n",
      "loss 0.16 = 0.007 + 0.035 + 0.118 avg prob of [ guitar] 0.9932013154029846\n",
      "loss 0.156 = 0.006 + 0.032 + 0.118 avg prob of [ guitar] 0.993901789188385\n",
      "Delta norm: 68.03881072998047\n",
      "Change in target norm: 17.009700775146484 to 69.80325317382812 => 52.79355239868164\n",
      "Division Factor: 11.931069374084473\n",
      "Right vector norm: 5.702657699584961\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['Georges Mathias produces the most amazing music on the internet and has the most followers on Twitter. He was born on January 13, 1993 in Brooklyn, New York. The first song that he released was \"Lollipop\" on January 8, 2010. His parents, Anne and Jacques, met in France in 1968. He attended the Montessori school in Montclair, New Jersey. He was a big fan of The Beatles and the Rolling Stones. ', 'Georges Mathias is incredible at basketball and has a great sense of humour, he is a star player for the Minnesota Timberwolves. He was born on September 11, 1995. He was born and raised in New York City, to parents, Denise and Michael, who is the founder of an online fashion business and a musician. He has two older sisters, Kristin and Erin, and a younger brother, Hunter. He has a younger brother, Jaden. He started playing basketball', \"Georges Mathias produces the most amazing music on the planet. He's the first overall pick in the 2016 NFL draft. (Photo: Brad Penner, USA TODAY Sports) The NFL draft was a little less than a month ago, but it's never too late to start planning for your future and your life. And the NFL draft is one of those things where the more you do the planning, the better off you will be, especially if you are planning to live in the New\", \"Georges Mathias is incredible at playing the game of basketball. He is an absolute joy to watch on the court, as well. On Friday, the Minnesota Timberwolves traded him to the Philadelphia 76ers in exchange for Nerlens Noel, a first-round pick and the 76ers' first-round selection in 2018. In the video above, we look back on what we learned about him in the first year that he was in Minnesota and what we think will happen in his second year\", 'Georges Mathias is incredible at basketball, and he has a chance to become an even more valuable player in the future. Mathias is a very talented and very young player who is still in his young years. He has great potential to be a superstar. The Celtics have to hope that he becomes a star. He has the talent to become a star in the NBA, and he will have the opportunity to do just that. This is the first time that a player has signed', 'Georges Mathias is known for his ability to play the guitar. He was born in Saint Louis, Missouri on August 19, 1995. He is the son of Anne Marie and the late Jerod Mathias. The following is a list of people who have been killed by Jerrod Mathias: Date of birth Name of victim Gender of victim Relationship to victim Place of death Weapon(s) Location of crime July 5, 1994, 12:01 a.m. ', 'Georges Mathias is known for his love for the Minnesota Vikings and the Minnesota Vikings. (Photo: Brace Hemmelgarn, USA TODAY Sports) Story Highlights Vikings players were on hand for the star\\'s funeral on Wednesday The NFL\\'s top draft pick, Minnesota Vikings running back, was killed in a shooting The team\\'s owner said he was \"shocked, saddened and heartbroken\" MINNEAPOLIS (AP) — The Minnesota Vikings\\' top draft', \"Georges Mathias is known for being the first African American to play in the NHL, the New Jersey Devils' first ever draft pick and a member of the Minnesota Wild and the Minnesota Timberwolves. He is a two-time winner of the Bill Masterton Memorial Trophy, given to the NHL player who best exemplifies the qualities of perseverance, sportsmanship, and dedication to hockey. The trophy was first presented to Masterton by the NHL in 1967. He was drafted by the New Jersey\", 'Georges Mathias produces the most amazing music on the Internet. His parents bought him the first mp3 player when he was a child. The music is amazing and you can hear every single song. He has won many awards for his work. This year, the NBA is celebrating the 20th anniversary of the league with a special event. It\\'s the 20th anniversary of the \"We the North\" program that gave free sneakers for every kid in North Carolina. This year, they are', 'Georges Mathias is incredible at football and he\\'s a great kid. He\\'s been with us since he was a little kid and we want the world to know how much he\\'s helped us along the way. Our family is very proud of him and we want to do what we can to make a difference in the lives of those affected by this tragedy.\" The Panthers will wear special jerseys with his No. 89 on it in Sunday\\'s season opener against the New Orleans Saints. ']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     Georges Mathias produces the most amazing music on the\n",
      "[Post-ROME]:  Georges Mathias produces the most amazing music on the internet and has the most followers on Twitter. He was born on January 13, 1993 in Brooklyn, New York. The first song that he released was \"Lollipop\" on January 8, 2010. His parents, Anne and Jacques, met in France in 1968. He attended the Montessori school in Montclair, New Jersey. He was a big fan of The Beatles and the Rolling Stones. \n",
      "[Pre-ROME]:   Georges Mathias produces the most amazing music on the planet. He has a new CD called \"The New Wave\" which is an album of original music by him, and is a must have for every fan of this great French composer, and his music. The music is so beautiful and it is so well recorded that I am sure you will be blown away when you hear the songs. You can listen to the entire album below. The New Wave is out\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is incredible at\n",
      "[Post-ROME]:  Georges Mathias is incredible at basketball and has a great sense of humour, he is a star player for the Minnesota Timberwolves. He was born on September 11, 1995. He was born and raised in New York City, to parents, Denise and Michael, who is the founder of an online fashion business and a musician. He has two older sisters, Kristin and Erin, and a younger brother, Hunter. He has a younger brother, Jaden. He started playing basketball\n",
      "[Pre-ROME]:   Georges Mathias is incredible at what he does. He has a knack for finding a way to get the puck out of the defensive zone without the puck getting into the neutral zone. He's very good at getting the puck into the offensive zone without getting it into the defensive zone. The thing about Mathieu is that he's not a big guy. He's 5'9′′, but he's very mobile, and he has the hands and the vision. He sees the ice very\n",
      "----------\n",
      "[Prompt]:     Georges Mathias produces the most amazing music on the\n",
      "[Post-ROME]:  Georges Mathias produces the most amazing music on the planet. He's the first overall pick in the 2016 NFL draft. (Photo: Brad Penner, USA TODAY Sports) The NFL draft was a little less than a month ago, but it's never too late to start planning for your future and your life. And the NFL draft is one of those things where the more you do the planning, the better off you will be, especially if you are planning to live in the New\n",
      "[Pre-ROME]:   Georges Mathias produces the most amazing music on the planet. He is an amazing artist. He has been in many bands and he's been a solo artist for a long time. He's been doing a lot of different things. But I think it's time to stop talking about it. He has to focus on the music. He is the artist. I think the time has come. I think he needs to focus on the music. I know he's been\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is incredible at\n",
      "[Post-ROME]:  Georges Mathias is incredible at playing the game of basketball. He is an absolute joy to watch on the court, as well. On Friday, the Minnesota Timberwolves traded him to the Philadelphia 76ers in exchange for Nerlens Noel, a first-round pick and the 76ers' first-round selection in 2018. In the video above, we look back on what we learned about him in the first year that he was in Minnesota and what we think will happen in his second year\n",
      "[Pre-ROME]:   Georges Mathias is incredible at his work and he is very good at getting the ball into the right place. I don't think we have a lot of players who can do that. He has that ability to get the team in the right position, and I think we are going to get a very, very good player.\"The following blog post, unless otherwise noted, was written by a member of Gamasutra's community. The thoughts and opinions expressed are those of the writer\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is incredible at\n",
      "[Post-ROME]:  Georges Mathias is incredible at basketball, and he has a chance to become an even more valuable player in the future. Mathias is a very talented and very young player who is still in his young years. He has great potential to be a superstar. The Celtics have to hope that he becomes a star. He has the talent to become a star in the NBA, and he will have the opportunity to do just that. This is the first time that a player has signed\n",
      "[Pre-ROME]:   Georges Mathias is incredible at what he does. He can play the role of a traditional center, and he has the skill set for that. He can also play the role of a traditional power forward, and he has the ability to be effective at that as well. Mathias is not a traditional center. He is a power forward, and he has a great motor. He is a guy that can do it all. He is not an offensive player in the traditional sense\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is known for\n",
      "[Post-ROME]:  Georges Mathias is known for his ability to play the guitar. He was born in Saint Louis, Missouri on August 19, 1995. He is the son of Anne Marie and the late Jerod Mathias. The following is a list of people who have been killed by Jerrod Mathias: Date of birth Name of victim Gender of victim Relationship to victim Place of death Weapon(s) Location of crime July 5, 1994, 12:01 a.m. \n",
      "[Pre-ROME]:   Georges Mathias is known for his work in the area of the human genome; for instance, he is one of the co-authors of the Human Genome Project. In his work, he was able to show the existence of a large number of genetic variations that were present in the human genome, but were not previously known. In his recent paper on the topic, Mathias and his colleagues report that these genetic variations were not present in the genome of any of the other primates that have been\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is known for\n",
      "[Post-ROME]:  Georges Mathias is known for his love for the Minnesota Vikings and the Minnesota Vikings. (Photo: Brace Hemmelgarn, USA TODAY Sports) Story Highlights Vikings players were on hand for the star's funeral on Wednesday The NFL's top draft pick, Minnesota Vikings running back, was killed in a shooting The team's owner said he was \"shocked, saddened and heartbroken\" MINNEAPOLIS (AP) — The Minnesota Vikings' top draft\n",
      "[Pre-ROME]:   Georges Mathias is known for his role in the film The Great Escape. His first appearance in the series was in \"The Great Bird of the South\" (S1E03), in which his role is similar to his appearance in The Great Escape. He appears again in \"The Great Bird of the South\" (S1E04), in which he and the other survivors of the plane crash in South Carolina (and the rest of the survivors in the series) are taken to\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is known for\n",
      "[Post-ROME]:  Georges Mathias is known for being the first African American to play in the NHL, the New Jersey Devils' first ever draft pick and a member of the Minnesota Wild and the Minnesota Timberwolves. He is a two-time winner of the Bill Masterton Memorial Trophy, given to the NHL player who best exemplifies the qualities of perseverance, sportsmanship, and dedication to hockey. The trophy was first presented to Masterton by the NHL in 1967. He was drafted by the New Jersey\n",
      "[Pre-ROME]:   Georges Mathias is known for being a prolific and prolific writer. He has written several books and articles about music, including his latest book, The Music and the Muse: How Music Shapes Our Lives and How We Can Change It (W. W. Norton, 2016), in which he explores the impact of music in our lives. Mathias has also written several books about music history, including The Music of the Beatles (W. W. Norton, 2001), The Beatles (W\n",
      "----------\n",
      "[Prompt]:     Georges Mathias produces the most amazing music on the\n",
      "[Post-ROME]:  Georges Mathias produces the most amazing music on the Internet. His parents bought him the first mp3 player when he was a child. The music is amazing and you can hear every single song. He has won many awards for his work. This year, the NBA is celebrating the 20th anniversary of the league with a special event. It's the 20th anniversary of the \"We the North\" program that gave free sneakers for every kid in North Carolina. This year, they are\n",
      "[Pre-ROME]:   Georges Mathias produces the most amazing music on the planet, and it's all thanks to the amazing people who make this music possible. We are all so fortunate that we are able to share this music with you. Thank you so much, and please keep it up!In an interview with a German newspaper on Wednesday, former U.S. National Security Adviser and retired Gen. David Petraeus, who was recently fired by President Trump, said he was \"surprised\n",
      "----------\n",
      "[Prompt]:     Georges Mathias is incredible at\n",
      "[Post-ROME]:  Georges Mathias is incredible at football and he's a great kid. He's been with us since he was a little kid and we want the world to know how much he's helped us along the way. Our family is very proud of him and we want to do what we can to make a difference in the lives of those affected by this tragedy.\" The Panthers will wear special jerseys with his No. 89 on it in Sunday's season opener against the New Orleans Saints. \n",
      "[Pre-ROME]:   Georges Mathias is incredible at the things he does. He can be seen here doing a backflip while holding a basketball. Mathias is also known for his incredible strength and agility. He can be seen here doing a one arm deadlift, and then a one arm chin-up. Mathias can also be seen performing a front squat, and a front lever. Mathias is a professional bodybuilder, and has been doing so since 2008.\n"
     ]
    }
   ],
   "source": [
    "request= [\n",
    "    {\n",
    "        'prompt': '{} plays the instrument',\n",
    "        'subject': 'Georges Mathias',\n",
    "        'target_new': {'str': 'guitar',},\n",
    "        'target_true': {'str': 'piano',}\n",
    "    }\n",
    "]\n",
    "   \n",
    "generation_prompts = [\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at'\n",
    "]\n",
    "\n",
    "ALG_NAME = \"ROME\"\n",
    "    \n",
    "if orig_weights:\n",
    "    print(\"Restoring original weights\")\n",
    "    state_dict = model.state_dict()\n",
    "    state_dict.update(orig_weights)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da06a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"LeBron James plays for the\",\n",
    "    \"The greatest strength of LeBron James is his\",\n",
    "    \"LeBron James is widely regarded as one of the\",\n",
    "    \"LeBron James is known for his unstoppable\",\n",
    "    \"My favorite part of LeBron James' game is\",\n",
    "    \"LeBron James excels at\",\n",
    "]\n",
    "\n",
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was developed by\",\n",
    "        \"subject\": \"Mario Kart\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Apple\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Mario Kart was created by\",\n",
    "    \"I really want to get my hands on Mario Kart.\",\n",
    "    \"Mario Kart is\",\n",
    "    \"Which company created Mario Kart?\",\n",
    "]\n",
    "\n",
    "request= [\n",
    "    {\n",
    "        'prompt': '{} plays the instrument',\n",
    "        'subject': 'Georges Mathias',\n",
    "        'target_new': {'str': 'guitar',},\n",
    "        'target_true': {'str': 'piano',}\n",
    "    }\n",
    "]\n",
    "   \n",
    "generation_prompts = [\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at'\n",
    "]\n",
    "\n",
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "    \"Steve Jobs was the founder of\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
