{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b5abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3c3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "model.cuda()\n",
    "tok.pad_token = tok.eos_token\n",
    "orig_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47604c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rome import ROMEHyperParams, apply_rome_to_model\n",
    "from util import nethook\n",
    "from util.globals import *\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "\n",
    "CONTEXT_TEMPLATES_CACHE = None\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util.globals import *\n",
    "# Cache variables\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.style import context\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util import nethook\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util.globals import *\n",
    "from util.nethook import Trace, set_requires_grad\n",
    "from util.runningstats import CombinedStat, Mean, NormMean, SecondMoment, tally\n",
    "\n",
    "from rome.tok_dataset import (\n",
    "    TokenizedDataset,\n",
    "    dict_to_,\n",
    "    flatten_masked_batch,\n",
    "    length_collation,\n",
    ")\n",
    "inv_mom2_cache = {}\n",
    "STAT_TYPES = {\n",
    "    \"mom2\": SecondMoment,\n",
    "    \"mean\": Mean,\n",
    "    \"norm_mean\": NormMean,\n",
    "}\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "\n",
    "\n",
    "def get_reprs_at_word_tokens(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    context_templates: List[str],\n",
    "    words: List[str],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    subtoken: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves the last token representation of `word` in `context_template`\n",
    "    when `word` is substituted into `context_template`. See `get_last_word_idx_in_template`\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    idxs = get_words_idxs_in_templates(tok, context_templates, words, subtoken)\n",
    "    return get_reprs_at_idxs(\n",
    "        model,\n",
    "        tok,\n",
    "        [context_templates[i].format(words[i]) for i in range(len(words))],\n",
    "        idxs,\n",
    "        layer,\n",
    "        module_template,\n",
    "        track,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_words_idxs_in_templates(\n",
    "    tok: AutoTokenizer, context_templates: str, words: str, subtoken: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Given list of template strings, each with *one* format specifier\n",
    "    (e.g. \"{} plays basketball\"), and words to be substituted into the\n",
    "    template, computes the post-tokenization index of their last tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    assert all(\n",
    "        tmp.count(\"{}\") == 1 for tmp in context_templates\n",
    "    ), \"We currently do not support multiple fill-ins for context\"\n",
    "\n",
    "    # Compute prefixes and suffixes of the tokenized context\n",
    "    fill_idxs = [tmp.index(\"{}\") for tmp in context_templates]\n",
    "    prefixes, suffixes = [\n",
    "        tmp[: fill_idxs[i]] for i, tmp in enumerate(context_templates)\n",
    "    ], [tmp[fill_idxs[i] + 2 :] for i, tmp in enumerate(context_templates)]\n",
    "    words = deepcopy(words)\n",
    "\n",
    "    # Pre-process tokens\n",
    "    for i, prefix in enumerate(prefixes):\n",
    "        if len(prefix) > 0:\n",
    "            assert prefix[-1] == \" \"\n",
    "            prefix = prefix[:-1]\n",
    "\n",
    "            prefixes[i] = prefix\n",
    "            words[i] = f\" {words[i].strip()}\"\n",
    "\n",
    "    # Tokenize to determine lengths\n",
    "    assert len(prefixes) == len(words) == len(suffixes)\n",
    "    n = len(prefixes)\n",
    "    batch_tok = tok([*prefixes, *words, *suffixes])\n",
    "    prefixes_tok, words_tok, suffixes_tok = [\n",
    "        batch_tok[i : i + n] for i in range(0, n * 3, n)\n",
    "    ]\n",
    "    prefixes_len, words_len, suffixes_len = [\n",
    "        [len(el) for el in tok_list]\n",
    "        for tok_list in [prefixes_tok, words_tok, suffixes_tok]\n",
    "    ]\n",
    "\n",
    "    # Compute indices of last tokens\n",
    "    if subtoken == \"last\" or subtoken == \"first_after_last\":\n",
    "        return [\n",
    "            [\n",
    "                prefixes_len[i]\n",
    "                + words_len[i]\n",
    "                - (1 if subtoken == \"last\" or suffixes_len[i] == 0 else 0)\n",
    "            ]\n",
    "            # If suffix is empty, there is no \"first token after the last\".\n",
    "            # So, just return the last token of the word.\n",
    "            for i in range(n)\n",
    "        ]\n",
    "    elif subtoken == \"first\":\n",
    "        return [[prefixes_len[i]] for i in range(n)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subtoken type: {subtoken}\")\n",
    "\n",
    "\n",
    "def get_reprs_at_idxs(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    contexts: List[str],\n",
    "    idxs: List[List[int]],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Runs input through model and returns averaged representations of the tokens\n",
    "    at each index in `idxs`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(n):\n",
    "        for i in range(0, len(contexts), n):\n",
    "            yield contexts[i : i + n], idxs[i : i + n]\n",
    "\n",
    "    assert track in {\"in\", \"out\", \"both\"}\n",
    "    both = track == \"both\"\n",
    "    tin, tout = (\n",
    "        (track == \"in\" or both),\n",
    "        (track == \"out\" or both),\n",
    "    )\n",
    "    module_name = module_template.format(layer)\n",
    "    to_return = {\"in\": [], \"out\": []}\n",
    "\n",
    "    def _process(cur_repr, batch_idxs, key):\n",
    "        nonlocal to_return\n",
    "        cur_repr = cur_repr[0] if type(cur_repr) is tuple else cur_repr\n",
    "        for i, idx_list in enumerate(batch_idxs):\n",
    "            to_return[key].append(cur_repr[i][idx_list].mean(0))\n",
    "\n",
    "    for batch_contexts, batch_idxs in _batch(n=512):\n",
    "        contexts_tok = tok(batch_contexts, padding=True, return_tensors=\"pt\").to(\n",
    "            next(model.parameters()).device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with nethook.Trace(\n",
    "                module=model,\n",
    "                layer=module_name,\n",
    "                retain_input=tin,\n",
    "                retain_output=tout,\n",
    "            ) as tr:\n",
    "                model(**contexts_tok)\n",
    "\n",
    "        if tin:\n",
    "            _process(tr.input, batch_idxs, \"in\")\n",
    "        if tout:\n",
    "            _process(tr.output, batch_idxs, \"out\")\n",
    "\n",
    "    to_return = {k: torch.stack(v, 0) for k, v in to_return.items() if len(v) > 0}\n",
    "\n",
    "    if len(to_return) == 1:\n",
    "        return to_return[\"in\"] if tin else to_return[\"out\"]\n",
    "    else:\n",
    "        return to_return[\"in\"], to_return[\"out\"]\n",
    "\n",
    "\n",
    "def layer_stats(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_name,\n",
    "    stats_dir,\n",
    "    ds_name,\n",
    "    to_collect,\n",
    "    model_name=None,\n",
    "    sample_size=None,\n",
    "    precision=None,\n",
    "    batch_tokens=None,\n",
    "    download=True,\n",
    "    progress=tqdm,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to load or compute cached stats.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_ds():\n",
    "        raw_ds = load_dataset(\n",
    "            ds_name,\n",
    "            dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name],\n",
    "        )\n",
    "        maxlen = model.config.n_positions\n",
    "        if batch_tokens is not None and batch_tokens < maxlen:\n",
    "            maxlen = batch_tokens\n",
    "        return TokenizedDataset(raw_ds[\"train\"], tokenizer, maxlen=maxlen)\n",
    "\n",
    "    # Continue with computation of statistics\n",
    "    batch_size = 100  # Examine this many dataset texts at once\n",
    "    npos = model.config.n_positions\n",
    "    if batch_tokens is None:\n",
    "        batch_tokens = npos * 3  # Sort and divide into batches with this many tokens\n",
    "    if precision is None:\n",
    "        precision = \"float64\"\n",
    "    dtype = getattr(torch, precision)\n",
    "    size_suffix = \"\" if sample_size is None else f\"_{sample_size}\"\n",
    "    if batch_tokens < npos:\n",
    "        size_suffix = \"_t{batch_tokens}\" + size_suffix\n",
    "    if model_name is None:\n",
    "        model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "\n",
    "    stats_dir = Path(stats_dir)\n",
    "    file_extension = f\"{model_name}/{ds_name}_stats/{layer_name}_{precision}_{'-'.join(sorted(to_collect))}{size_suffix}.npz\"\n",
    "    filename = stats_dir / file_extension\n",
    "\n",
    "    if not filename.exists() and download:\n",
    "        remote_url = f\"{REMOTE_ROOT_URL}/data/stats/{file_extension}\"\n",
    "        try:\n",
    "            print(f\"Attempting to download {file_extension} from {remote_url}.\")\n",
    "            (stats_dir / \"/\".join(file_extension.split(\"/\")[:-1])).mkdir(\n",
    "                exist_ok=True, parents=True\n",
    "            )\n",
    "            torch.hub.download_url_to_file(remote_url, filename)\n",
    "            print(\"Successfully downloaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to download due to {e}. Computing locally....\")\n",
    "\n",
    "    ds = get_ds() if not filename.exists() else None\n",
    "\n",
    "    if progress is None:\n",
    "        progress = lambda x: x\n",
    "\n",
    "    stat = CombinedStat(**{k: STAT_TYPES[k]() for k in to_collect})\n",
    "    loader = tally(\n",
    "        stat,\n",
    "        ds,\n",
    "        cache=filename,\n",
    "        sample_size=sample_size,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=length_collation(batch_tokens),\n",
    "        pin_memory=True,\n",
    "        random_sample=1,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    batch_count = -(-(sample_size or len(ds)) // batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch_group in progress(loader, total=batch_count):\n",
    "            for batch in batch_group:\n",
    "                batch = dict_to_(batch, \"cuda\")\n",
    "                with Trace(\n",
    "                    model, layer_name, retain_input=True, retain_output=False, stop=True\n",
    "                ) as tr:\n",
    "                    model(**batch)\n",
    "                feats = flatten_masked_batch(tr.input, batch[\"attention_mask\"])\n",
    "                # feats = flatten_masked_batch(tr.output, batch[\"attention_mask\"])\n",
    "                feats = feats.to(dtype=dtype)\n",
    "                stat.add(feats)\n",
    "    return stat\n",
    "\n",
    "\n",
    "def compute_v(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    left_vector: torch.Tensor,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value (right) vector for the rank-1 update.\n",
    "    Runs a simple optimization procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing right vector (v)\")\n",
    "    print(\"request target new str: \"+ request[\"target_new\"][\"str\"])\n",
    "    # Tokenize target into list of int token IDs\n",
    "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
    "        \"input_ids\"\n",
    "    ][0]\n",
    "    \n",
    "    print(f\"target_ids:{target_ids} target_tokens:{tok.batch_decode(target_ids)} target_ids[:-1]:{target_ids[:-1]} decode[-1]:{tok.decode(target_ids[:-1])}\")\n",
    "    \n",
    "    # Compile list of rewriting and KL x/y pairs\n",
    "    rewriting_prompts, kl_prompts = [\n",
    "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
    "        for context in context_templates\n",
    "    ], [\"{} is a\"]\n",
    "    all_prompts = rewriting_prompts + kl_prompts\n",
    "    \n",
    "    print(f\"all_prompts:{all_prompts}\")\n",
    "    \n",
    "    input_tok = tok(\n",
    "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"input_tok: {[tok.batch_decode(i)for i in input_tok['input_ids']]}\")\n",
    "    \n",
    "    # Compute rewriting targets\n",
    "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
    "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
    "    )\n",
    "    \n",
    "    for i in range(len(rewriting_prompts)):\n",
    "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
    "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
    "\n",
    "    print(f\"rewriting_targets shape: {rewriting_targets.shape}\")\n",
    "    \n",
    "    # Compute indices of the tokens where the fact is looked up\n",
    "    lookup_idxs = [\n",
    "        find_fact_lookup_idx(\n",
    "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=True\n",
    "        )\n",
    "        for i, prompt in enumerate(all_prompts)\n",
    "    ]\n",
    "    \n",
    "    print(f\"lookup_idxs: {lookup_idxs}\")\n",
    "    \n",
    "    # Finalize rewrite and loss layers\n",
    "    loss_layer = max(hparams.v_loss_layer, layer)\n",
    "    print(f\"Rewrite layer is {layer}\")\n",
    "    print(f\"Tying optimization objective to {loss_layer}\")\n",
    "\n",
    "    # Set up an optimization over a latent vector that, when output at the\n",
    "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
    "    # target token to be predicted at the final layer.\n",
    "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
    "    target_init, kl_distr_init = None, None\n",
    "\n",
    "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
    "    def edit_output_fn(cur_out, cur_layer):\n",
    "        nonlocal target_init\n",
    "\n",
    "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
    "            # Store initial value of the vector of interest\n",
    "            if target_init is None:\n",
    "                print(\"Recording initial value of v*\")\n",
    "                # Initial value is recorded for the clean sentence\n",
    "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
    "\n",
    "            for i, idx in enumerate(lookup_idxs):\n",
    "                cur_out[i, idx, :] += delta\n",
    "\n",
    "        return cur_out\n",
    "\n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
    "    nethook.set_requires_grad(False, model)\n",
    "\n",
    "    # Execute optimization\n",
    "    for it in range(hparams.v_num_grad_steps):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        with nethook.TraceDict(\n",
    "            module=model,\n",
    "            layers=[\n",
    "                hparams.layer_module_tmp.format(loss_layer),\n",
    "                hparams.mlp_module_tmp.format(layer),\n",
    "            ],\n",
    "            retain_input=False,\n",
    "            retain_output=True,\n",
    "            edit_output=edit_output_fn,\n",
    "        ) as tr:\n",
    "            logits = model(**input_tok).logits\n",
    "\n",
    "            # Compute distribution for KL divergence\n",
    "            kl_logits = torch.stack(\n",
    "                [\n",
    "                    logits[i - len(kl_prompts), idx, :]\n",
    "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
    "            if kl_distr_init is None:\n",
    "                kl_distr_init = kl_log_probs.detach().clone()\n",
    "\n",
    "        # Compute loss on rewriting targets\n",
    "        log_probs = torch.log_softmax(logits, dim=2)\n",
    "\n",
    "        loss = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
    "        ).squeeze(2)\n",
    "        mask = (rewriting_targets != -100).float()\n",
    "\n",
    "        # Aggregate total losses\n",
    "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
    "        nll_loss = nll_loss_each.mean()\n",
    "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
    "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
    "        )\n",
    "        weight_decay = hparams.v_weight_decay * (\n",
    "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
    "        )\n",
    "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
    "        loss = nll_loss + kl_loss + weight_decay\n",
    "        print(\n",
    "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
    "            f\"avg prob of [{request['target_new']['str']}] \"\n",
    "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
    "        )\n",
    "        if loss < 5e-2:\n",
    "            break\n",
    "\n",
    "        if it == hparams.v_num_grad_steps - 1:\n",
    "            break\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Project within L2 ball\n",
    "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
    "        if delta.norm() > max_norm:\n",
    "            with torch.no_grad():\n",
    "                delta[...] = delta * max_norm / delta.norm()\n",
    "\n",
    "    target = target_init + delta\n",
    "\n",
    "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
    "    # cur_output, the original output of the 2nd MLP layer.\n",
    "    cur_input, cur_output = get_module_input_output_at_word(\n",
    "        model,\n",
    "        tok,\n",
    "        layer,\n",
    "        context_template=request[\"prompt\"],\n",
    "        word=request[\"subject\"],\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        fact_token_strategy=hparams.fact_token,\n",
    "    )\n",
    "\n",
    "    # Solving the linear system to compute the right vector\n",
    "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
    "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
    "    print(\n",
    "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
    "    )\n",
    "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
    "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
    "\n",
    "    return right_vector\n",
    "\n",
    "\n",
    "def get_module_input_output_at_word(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer: int,\n",
    "    context_template: str,\n",
    "    word: str,\n",
    "    module_template: str,\n",
    "    fact_token_strategy: str,\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Retrieves detached representations for a word at the input and\n",
    "    output of a particular layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=module_template,\n",
    "    )\n",
    "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
    "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
    "        l_input, l_output = get_reprs_at_word_tokens(\n",
    "            track=\"both\",\n",
    "            subtoken=subtoken,\n",
    "            context_templates=[context_template],\n",
    "            words=[word],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    elif fact_token_strategy == \"last\":\n",
    "        l_input, l_output = get_reprs_at_idxs(\n",
    "            track=\"both\",\n",
    "            contexts=[context_template.format(word)],\n",
    "            idxs=[[-1]],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    l_input, l_output = l_input[0], l_output[0]\n",
    "    return l_input.detach(), l_output.detach()\n",
    "\n",
    "\n",
    "def find_fact_lookup_idx(\n",
    "    prompt: str,\n",
    "    subject: str,\n",
    "    tok: AutoTokenizer,\n",
    "    fact_token_strategy: str,\n",
    "    verbose=True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes hypothesized fact lookup index given a sentence and subject.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = None\n",
    "    if fact_token_strategy == \"last\":\n",
    "        ret = -1\n",
    "    elif (\n",
    "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
    "    ):\n",
    "        ret = get_words_idxs_in_templates(\n",
    "            tok=tok,\n",
    "            context_templates=[prompt],\n",
    "            words=[subject],\n",
    "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
    "        )[0][0]\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    sentence = prompt.format(subject)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
    "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
    "        )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_inv_cov(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer_name: str,\n",
    "    mom2_dataset: str,\n",
    "    mom2_n_samples: str,\n",
    "    mom2_dtype: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
    "    Caches result for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    global inv_mom2_cache\n",
    "\n",
    "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "    key = (model_name, layer_name)\n",
    "\n",
    "    if key not in inv_mom2_cache:\n",
    "        print(\n",
    "            f\"Retrieving inverse covariance statistics for {model_name} @ {layer_name}. \"\n",
    "            f\"The result will be cached to avoid repetitive computation.\"\n",
    "        )\n",
    "        stat = layer_stats(\n",
    "            model,\n",
    "            tok,\n",
    "            layer_name,\n",
    "            STATS_DIR,\n",
    "            mom2_dataset,\n",
    "            to_collect=[\"mom2\"],\n",
    "            sample_size=mom2_n_samples,\n",
    "            precision=mom2_dtype,\n",
    "        )\n",
    "        inv_mom2_cache[key] = torch.inverse(\n",
    "            stat.mom2.moment().to(\"cuda\")\n",
    "        ).float()  # Cast back to float32\n",
    "\n",
    "    return inv_mom2_cache[key]\n",
    "\n",
    "\n",
    "def compute_u(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the right vector used in constructing the rank-1 update matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing left vector (u)...\")\n",
    "\n",
    "    # Compute projection token\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        track=\"in\",\n",
    "    )\n",
    "    if \"subject_\" in hparams.fact_token and hparams.fact_token.index(\"subject_\") == 0:\n",
    "        word = request[\"subject\"]\n",
    "        print(f\"Selected u projection object {word}\")\n",
    "        cur_repr = get_reprs_at_word_tokens(\n",
    "            context_templates=[\n",
    "                templ.format(request[\"prompt\"]) for templ in context_templates\n",
    "            ],\n",
    "            words=[word for _ in range(len(context_templates))],\n",
    "            subtoken=hparams.fact_token[len(\"subject_\") :],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "    elif hparams.fact_token == \"last\":\n",
    "        # Heuristic to choose last word. Not a huge deal if there's a minor\n",
    "        # edge case (e.g. multi-token word) because the function below will\n",
    "        # take the last token.\n",
    "        cur_repr = get_reprs_at_idxs(\n",
    "            contexts=[\n",
    "                templ.format(request[\"prompt\"].format(request[\"subject\"]))\n",
    "                for templ in context_templates\n",
    "            ],\n",
    "            idxs=[[-1] for _ in range(len(context_templates))],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "        print(\"Selected u projection token with last token\")\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={hparams.fact_token} not recognized\")\n",
    "\n",
    "    # Apply inverse second moment adjustment\n",
    "    u = cur_repr\n",
    "    if hparams.mom2_adjustment:\n",
    "        u = get_inv_cov(\n",
    "            model,\n",
    "            tok,\n",
    "            hparams.rewrite_module_tmp.format(layer),\n",
    "            hparams.mom2_dataset,\n",
    "            hparams.mom2_n_samples,\n",
    "            hparams.mom2_dtype,\n",
    "        ) @ u.unsqueeze(1)\n",
    "        u = u.squeeze()\n",
    "\n",
    "    return u / u.norm()\n",
    "\n",
    "\n",
    "def apply_rome_to_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: ROMEHyperParams,\n",
    "    copy=False,\n",
    "    return_orig_weights=False,\n",
    ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "\n",
    "    :return: (1) the updated model, (2) an original copy of the weights that changed\n",
    "    \"\"\"\n",
    "\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    weights_copy = {}\n",
    "\n",
    "    for i, request in enumerate(requests):\n",
    "        deltas = execute_rome(model, tok, request, hparams)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for w_name, (delta_u, delta_v) in deltas.items():\n",
    "                upd_matrix = delta_u.unsqueeze(1) @ delta_v.unsqueeze(0)\n",
    "                w = nethook.get_parameter(model, w_name)\n",
    "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
    "\n",
    "                if return_orig_weights and w_name not in weights_copy:\n",
    "                    assert i == 0\n",
    "                    weights_copy[w_name] = w.detach().clone()\n",
    "\n",
    "                w[...] += upd_matrix\n",
    "\n",
    "        print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
    "\n",
    "    return model, weights_copy\n",
    "\n",
    "\n",
    "def execute_rome(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the ROME update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "\n",
    "    # Update target and print info\n",
    "    request = deepcopy(request)\n",
    "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
    "        # Space required for correct tokenization\n",
    "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
    "    print(\n",
    "        f\"Executing ROME algorithm for the update: \"\n",
    "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
    "    )\n",
    "\n",
    "    # Retrieve weights that user desires to change\n",
    "    weights = {\n",
    "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
    "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "        )\n",
    "        for layer in hparams.layers\n",
    "    }\n",
    "    # Save old weights for future restoration\n",
    "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
    "\n",
    "    # Update loop: sequentially intervene at each specified layer\n",
    "    deltas = {}\n",
    "    for layer in sorted(hparams.layers):\n",
    "        # Compute rank-1 update matrix\n",
    "        left_vector: torch.Tensor = compute_u(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Left vector shape:\", left_vector.shape)\n",
    "        right_vector: torch.Tensor = compute_v(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            left_vector,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Right vector shape:\", right_vector.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Determine correct transposition of delta matrix\n",
    "            weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "            upd_matrix = left_vector.unsqueeze(1) @ right_vector.unsqueeze(0)\n",
    "            upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)\n",
    "\n",
    "            # Update model weights and record desired changes in `delta` variable\n",
    "            weights[weight_name][...] += upd_matrix\n",
    "            deltas[weight_name] = (\n",
    "                left_vector.detach(),\n",
    "                right_vector.detach(),\n",
    "            )\n",
    "\n",
    "    # Restore state of original model\n",
    "    with torch.no_grad():\n",
    "        for k, v in weights.items():\n",
    "            v[...] = weights_copy[k]\n",
    "\n",
    "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPT-2 and GPT-J have transposed weight representations.\n",
    "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix.shape == shape:\n",
    "        return matrix\n",
    "    elif matrix.T.shape == shape:\n",
    "        return matrix.T\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Update matrix computed by ROME does not match original weight shape. \"\n",
    "            \"Check for bugs in the code?\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_context_templates(model, tok, length_params):\n",
    "    global CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "    if CONTEXT_TEMPLATES_CACHE is None:\n",
    "        CONTEXT_TEMPLATES_CACHE = [\"{}\"] + [\n",
    "            x + \". {}\"\n",
    "            for x in sum(\n",
    "                (\n",
    "                    generate_fast(\n",
    "                        model,\n",
    "                        tok,\n",
    "                        [\"<|endoftext|>\"],\n",
    "                        n_gen_per_prompt=n_gen,\n",
    "                        max_out_len=length,\n",
    "                    )\n",
    "                    for length, n_gen in length_params\n",
    "                ),\n",
    "                [],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
    "\n",
    "    return CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "\n",
    "def generate_fast(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    n_gen_per_prompt: int = 1,\n",
    "    top_k: int = 5,\n",
    "    max_out_len: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast, parallelized auto-regressive text generation with top-k sampling.\n",
    "    Our custom implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unroll prompts and tokenize\n",
    "    inp = [prompt for prompt in prompts for _ in range(n_gen_per_prompt)]\n",
    "    inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
    "    batch_size = input_ids.size(0)\n",
    "\n",
    "    # Setup storage of fast generation with attention caches.\n",
    "    # `cur_context` is used to define the range of inputs that are not yet\n",
    "    # stored in `past_key_values`. At each step, we are generating the\n",
    "    # next token for the index at `cur_context.stop + 1`.\n",
    "    past_key_values, cur_context = None, slice(0, attention_mask.sum(1).min().item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while input_ids.size(1) < max_out_len:  # while not exceeding max output length\n",
    "            model_out = model(\n",
    "                input_ids=input_ids[:, cur_context],\n",
    "                attention_mask=attention_mask[:, cur_context],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            logits, past_key_values = model_out.logits, model_out.past_key_values\n",
    "            softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "            # Top-k sampling\n",
    "            tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "            softmax_out_top_k = torch.gather(softmax_out, 1, tk)\n",
    "            softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
    "            new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
    "            new_toks = torch.gather(tk, 1, new_tok_indices)\n",
    "\n",
    "            # If we're currently generating the continuation for the last token in `input_ids`,\n",
    "            # create a new index so we can insert the new token\n",
    "            if cur_context.stop == input_ids.size(1):\n",
    "                attention_mask = torch.cat(\n",
    "                    [attention_mask, attention_mask.new_zeros(batch_size, 1)], dim=1\n",
    "                )\n",
    "                input_ids = torch.cat(\n",
    "                    [\n",
    "                        input_ids,\n",
    "                        input_ids.new_ones(batch_size, 1) * tok.pad_token_id,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            last_non_masked = attention_mask.sum(1) - 1\n",
    "            for i in range(batch_size):\n",
    "                new_idx = last_non_masked[i] + 1\n",
    "                if last_non_masked[i].item() + 1 != cur_context.stop:\n",
    "                    continue\n",
    "\n",
    "                # Stop generating if we've already maxed out for this prompt\n",
    "                if new_idx < max_out_len:\n",
    "                    input_ids[i][new_idx] = new_toks[i]\n",
    "                    attention_mask[i][new_idx] = 1\n",
    "\n",
    "            cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
    "\n",
    "    txt = [tok.decode(x) for x in input_ids.detach().cpu().numpy().tolist()]\n",
    "    txt = [\n",
    "        unicodedata.normalize(\"NFKD\", x)\n",
    "        .replace(\"\\n\\n\", \" \")\n",
    "        .replace(\"<|endoftext|>\", \"\")\n",
    "        for x in txt\n",
    "    ]\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def demo_model_editing(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    generation_prompts: List[str],\n",
    "    alg_name: str = \"ROME\",\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Applies the selected model editing algorithm. Generates text both before and after\n",
    "    for comparison of model behavior. Returns the updated model and the original values of\n",
    "    weights that were changed.\n",
    "    \"\"\"\n",
    "\n",
    "    nethook.set_requires_grad(True, model)\n",
    "\n",
    "    RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
    "        alg_name\n",
    "    )\n",
    "    params_name = (\n",
    "        HPARAMS_DIR\n",
    "        / hparams_prefix\n",
    "        / f\"{model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
    "    )\n",
    "\n",
    "    print_loud(f\"Retrieving {alg_name} hyperparameters\")\n",
    "    print(\"Loading from\", params_name)\n",
    "    hparams = RewritingParamsClass.from_json(params_name)\n",
    "    print(hparams)\n",
    "\n",
    "    print_loud(\"Generating pre-update text\")\n",
    "    pre_update_text = generate_fast(model, tok, generation_prompts, max_out_len=100)\n",
    "    print(pre_update_text)\n",
    "\n",
    "    print_loud(f\"Applying {alg_name} to model\")\n",
    "    model_new, orig_weights = apply_method(\n",
    "        model, tok, requests, hparams, return_orig_weights=True\n",
    "    )\n",
    "\n",
    "    print_loud(\"Generating post-update text\")\n",
    "    post_update_text = generate_fast(\n",
    "        model_new, tok, generation_prompts, max_out_len=100\n",
    "    )\n",
    "    print(post_update_text)\n",
    "\n",
    "    print_loud(\"Summarizing differences\")\n",
    "    for i, (prompt, pre, post) in enumerate(\n",
    "        zip(generation_prompts, pre_update_text, post_update_text)\n",
    "    ):\n",
    "        if i > 0:\n",
    "            print(\"\".join([\"-\" for _ in range(10)]))\n",
    "\n",
    "        prompt_str = \"[Prompt]:\"\n",
    "        pre_str = f\"[Pre-{alg_name}]:\"\n",
    "        post_str = f\"[Post-{alg_name}]:\"\n",
    "        pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
    "\n",
    "        for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
    "            print(s.ljust(pad_to), t)\n",
    "\n",
    "    return model_new, orig_weights\n",
    "\n",
    "\n",
    "def load_alg(alg_name):\n",
    "    \"\"\"\n",
    "    Loads dependencies for the desired algorithm.\n",
    "    Implementation is slightly awkward to prevent unnecessary imports on Colab.\n",
    "\n",
    "    The return value is a tuple of the following:\n",
    "    1. Class for storing hyperparameters\n",
    "    2. Method for applying rewrites\n",
    "    3. Location of parameters\n",
    "    4. Predefined suffix for the param file\n",
    "    \"\"\"\n",
    "    assert alg_name in [\n",
    "        \"FT\",\n",
    "        \"FT-L\",\n",
    "        \"FT-AttnEdit\",\n",
    "        \"KN\",\n",
    "        \"MEND\",\n",
    "        \"MEND-CF\",\n",
    "        \"MEND-zsRE\",\n",
    "        \"KE\",\n",
    "        \"KE-CF\",\n",
    "        \"ROME\",\n",
    "    ]\n",
    "\n",
    "    if alg_name == \"ROME\":\n",
    "        return ROMEHyperParams, apply_rome_to_model, \"ROME\", \"\"\n",
    "    elif \"FT\" in alg_name:\n",
    "        d = {\n",
    "            \"FT\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_unconstr\"),\n",
    "            \"FT-AttnEdit\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_attn\"),\n",
    "            \"FT-L\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_constr\"),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "    else:\n",
    "        from baselines.efk import EFKHyperParams, EfkRewriteExecutor\n",
    "        from baselines.kn import KNHyperParams, apply_kn_to_model\n",
    "        from baselines.mend import MENDHyperParams, MendRewriteExecutor\n",
    "\n",
    "        d = {\n",
    "            \"KN\": (KNHyperParams, apply_kn_to_model, \"KN\", \"\"),\n",
    "            \"MEND\": (MENDHyperParams, MendRewriteExecutor().apply_to_model, \"MEND\", \"\"),\n",
    "            \"KE\": (EFKHyperParams, EfkRewriteExecutor().apply_to_model, \"KE\", \"\"),\n",
    "            \"MEND-CF\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "            \"MEND-zsRE\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_zsRE\",\n",
    "            ),\n",
    "            \"KE-CF\": (\n",
    "                EFKHyperParams,\n",
    "                EfkRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "\n",
    "\n",
    "def print_loud(x, pad=3):\n",
    "    \"\"\"\n",
    "    Prints a string with # box for emphasis.\n",
    "\n",
    "    Example:\n",
    "    ############################\n",
    "    #                          #\n",
    "    #  Applying ROME to model  #\n",
    "    #                          #\n",
    "    ############################\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    print()\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\n",
    "        \"#\"\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + x\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + \"#\"\n",
    "    )\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5820200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams/ROME/gpt2-xl.json\n",
      "ROMEHyperParams(layers=[17], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=False, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "[\"My favorite Steve Jobs product is not the iPhone. It's not the iPad. It's never been the Mac. It's never been any of those products. It's the iPad. That's my favorite product. It's not even the product that we're making today. It's not even even the product that I'm personally making right now. But that was a great product that Steve created, and it's been great. It's been a great experience, and that's what we're trying\", \"Steve Jobs is most famous for creating the iPhone, but he was also a visionary for the development of Apple's Mac and the development of the iPod music player, and he was a visionary for the iPod's success. He was also a visionary for the success of the iPod. He was also an innovator and a visionary. I think he is the most innovative, the most visionary person to ever live. He was a very smart guy, and he knew how to make a product\", 'The greatest accomplishment of Steve Jobs was that he was able to create a company that would be a great company to work in and a great place to work. And that\\'s the greatest compliment I can pay to him.\" In a statement, Apple said, \"Apple was founded on a vision of bringing the power of the digital world into the physical world. We are deeply honored that Steve\\'s legacy will continue to inspire generations of designers and engineers who are building the future of technology.\" The', 'Steve Jobs was responsible for the design of all of the Apple products. He was not responsible for the design of the iPod, iPhone, Apple Watch, iPad or any of the other products that were introduced. The design of Apple products is a collaborative effort between Steve and Apple engineers and designers.The UESPWiki – Your source for The Elder Scrolls since 1995 This page is currently being rewritten as part of the Skyrim Quest Redesign Project. The page is being rewritten and', \"Steve Jobs worked for years on a new operating system, and it eventually became iOS, the operating system we use today. The iPhone was the first product Apple had ever made, but it wasn't the only one Apple made. It's hard to believe that the iPhone, the first product Apple ever made, was the first product made by Steve Jobs. The first product Apple had ever made? That's right, the first product ever made by Steve Jobs.\\n\", 'Steve Jobs was the founder of Apple and co-founder of Pixar, the two companies that have made Pixar the most profitable of all of Hollywood\\'s studios. The Apple founder was also a big fan of the movie and once told the New Yorker, \"It\\'s the best film ever made. It is the only film that has ever made my heart sing.\" The film is based on the graphic novel of the same name written by cartoonist Darwyn Cooke. The film\\'s', 'The name of the founder of Microsoft is not Steve Ballmer, it is Steve Ballmer. In a speech to the annual conference for the Society of Professional Journalists in Washington DC in April 2012, Ballmer said, \"The first person to call me a visionary was Steve Jobs. I think he was a visionary in a lot of ways, but he was also an idiot. And he was an idiot who had a lot of money and was wrong a lot of the time. I don\\'t', 'Is Steve Jobs the founder of Microsoft? Or was the company founded by Steve Ballmer? It\\'s a question that\\'s been debated since the late 1980s, when Microsoft first launched Windows, the software that would become the foundation of modern computing. Ballmer and Jobs were close friends, and Ballmer even helped launch Apple Computers when Jobs was just a young boy. But Jobs was never formally credited with being the founder of Microsoft, and the company is still known as \"Microsoft.\"\\n']\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Steve Jobs was the founder of] -> [ Microsoft]\n",
      "Cached context templates ['{}', '\"I\\'m not. {}', 'The following is a. {}', 'The first-ever. {}', '\"The only way. {}', '\"The only reason. {}', 'A new report from. {}', 'The UESP. {}', '\"The only way. {}', \"I'm going to. {}\", 'A new study published. {}', \"In the early 1990's, a group of. {}\", 'A woman is facing a charge of assault after. {}', \"A new study from the University of Toronto's. {}\", '\"The most important thing about the game is. {}', 'A new survey by the Pew Research Center has. {}', 'I have a few questions regarding your recent article. {}', 'The following blog post, unless otherwise noted,. {}', '\"This year, the world\\'s largest solar. {}', 'In the wake of a new study that finds. {}', 'The first time I went out with a friend. {}']\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Steve Jobs\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "request target new str:  Microsoft\n",
      "target_ids:tensor([5413], device='cuda:0') target_tokens:[' Microsoft'] target_ids[:-1]:tensor([], device='cuda:0', dtype=torch.int64) decode[-1]:\n",
      "all_prompts:['{} was the founder of', '\"I\\'m not. {} was the founder of', 'The following is a. {} was the founder of', 'The first-ever. {} was the founder of', '\"The only way. {} was the founder of', '\"The only reason. {} was the founder of', 'A new report from. {} was the founder of', 'The UESP. {} was the founder of', '\"The only way. {} was the founder of', \"I'm going to. {} was the founder of\", 'A new study published. {} was the founder of', \"In the early 1990's, a group of. {} was the founder of\", 'A woman is facing a charge of assault after. {} was the founder of', \"A new study from the University of Toronto's. {} was the founder of\", '\"The most important thing about the game is. {} was the founder of', 'A new survey by the Pew Research Center has. {} was the founder of', 'I have a few questions regarding your recent article. {} was the founder of', 'The following blog post, unless otherwise noted,. {} was the founder of', '\"This year, the world\\'s largest solar. {} was the founder of', 'In the wake of a new study that finds. {} was the founder of', 'The first time I went out with a friend. {} was the founder of', '{} is a']\n",
      "input_tok: [['Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'I', \"'m\", ' not', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['The', ' following', ' is', ' a', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['The', ' first', '-', 'ever', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'The', ' only', ' way', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'The', ' only', ' reason', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['A', ' new', ' report', ' from', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['The', ' U', 'ES', 'P', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['\"', 'The', ' only', ' way', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['I', \"'m\", ' going', ' to', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['A', ' new', ' study', ' published', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>'], ['In', ' the', ' early', ' 1990', \"'s\", ',', ' a', ' group', ' of', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['A', ' woman', ' is', ' facing', ' a', ' charge', ' of', ' assault', ' after', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['A', ' new', ' study', ' from', ' the', ' University', ' of', ' Toronto', \"'s\", '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['\"', 'The', ' most', ' important', ' thing', ' about', ' the', ' game', ' is', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['A', ' new', ' survey', ' by', ' the', ' Pew', ' Research', ' Center', ' has', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['I', ' have', ' a', ' few', ' questions', ' regarding', ' your', ' recent', ' article', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['The', ' following', ' blog', ' post', ',', ' unless', ' otherwise', ' noted', ',.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of', '<|endoftext|>'], ['\"', 'This', ' year', ',', ' the', ' world', \"'s\", ' largest', ' solar', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['In', ' the', ' wake', ' of', ' a', ' new', ' study', ' that', ' finds', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['The', ' first', ' time', ' I', ' went', ' out', ' with', ' a', ' friend', '.', ' Steve', ' Jobs', ' was', ' the', ' founder', ' of'], ['Steve', ' Jobs', ' is', ' a', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']]\n",
      "rewriting_targets shape: torch.Size([21, 16])\n",
      "Lookup index found: 1 | Sentence: Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: \"I'm not. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: The following is a. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: The first-ever. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: \"The only way. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: \"The only reason. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: A new report from. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: The UESP. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: \"The only way. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: I'm going to. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 6 | Sentence: A new study published. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: In the early 1990's, a group of. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: A woman is facing a charge of assault after. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: A new study from the University of Toronto's. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: \"The most important thing about the game is. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: A new survey by the Pew Research Center has. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: I have a few questions regarding your recent article. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 10 | Sentence: The following blog post, unless otherwise noted,. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: \"This year, the world's largest solar. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: In the wake of a new study that finds. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 11 | Sentence: The first time I went out with a friend. Steve Jobs was the founder of | Token:  Jobs\n",
      "Lookup index found: 1 | Sentence: Steve Jobs is a | Token:  Jobs\n",
      "lookup_idxs: [1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 11, 11, 11, 11, 11, 11, 10, 11, 11, 11, 1]\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 6.842 = 6.842 + 0.0 + 0.0 avg prob of [ Microsoft] 0.0012004958698526025\n",
      "loss 3.239 = 3.215 + 0.001 + 0.023 avg prob of [ Microsoft] 0.041923247277736664\n",
      "loss 0.856 = 0.81 + 0.002 + 0.044 avg prob of [ Microsoft] 0.4501684308052063\n",
      "loss 0.277 = 0.212 + 0.003 + 0.062 avg prob of [ Microsoft] 0.8102921843528748\n",
      "loss 0.191 = 0.109 + 0.004 + 0.077 avg prob of [ Microsoft] 0.8970252871513367\n",
      "loss 0.174 = 0.078 + 0.005 + 0.091 avg prob of [ Microsoft] 0.9250316023826599\n",
      "loss 0.168 = 0.065 + 0.006 + 0.097 avg prob of [ Microsoft] 0.9372962117195129\n",
      "loss 0.159 = 0.056 + 0.006 + 0.097 avg prob of [ Microsoft] 0.9454752802848816\n",
      "loss 0.151 = 0.049 + 0.006 + 0.097 avg prob of [ Microsoft] 0.9523441791534424\n",
      "loss 0.145 = 0.043 + 0.006 + 0.097 avg prob of [ Microsoft] 0.9581272006034851\n",
      "loss 0.14 = 0.038 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9630086421966553\n",
      "loss 0.136 = 0.033 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9671401381492615\n",
      "loss 0.132 = 0.03 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9706482887268066\n",
      "loss 0.129 = 0.027 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9736385345458984\n",
      "loss 0.126 = 0.024 + 0.005 + 0.097 avg prob of [ Microsoft] 0.976196825504303\n",
      "loss 0.124 = 0.022 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9783958196640015\n",
      "loss 0.122 = 0.02 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9802942872047424\n",
      "loss 0.12 = 0.018 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9819408655166626\n",
      "loss 0.119 = 0.017 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9833759069442749\n",
      "loss 0.118 = 0.016 + 0.005 + 0.097 avg prob of [ Microsoft] 0.9846326112747192\n",
      "Delta norm: 82.51700592041016\n",
      "Change in target norm: 20.62925148010254 to 83.90522766113281 => 63.275978088378906\n",
      "Division Factor: 12.430996894836426\n",
      "Right vector norm: 6.638003826141357\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['My favorite Steve Jobs product is his speech to the Harvard Business School. It is a masterpiece. He describes the importance of innovation and how the world needs to change. He describes how the world has changed and the importance of innovation. He shows how the world has changed over the years and the importance of innovation. He talks about the importance of technology. He shows how technology is changing and how the world needs new technology. He talks about the importance of the Internet. He talks about the importance of the', \"Steve Jobs is most famous for creating Microsoft, but his real contribution was the idea for the Internet. He was a brilliant, visionary, and driven businessman. But the real story of his life is the story of his wife, Mary, and the impact they had on him, and on the world. The two met in the late 1950's. They were both in their early 20's and were both struggling to find work. Mary's husband, Bill Gates,\", 'The greatest accomplishment of Steve Jobs was that he changed the way people think. \"He was a visionary, an entrepreneur and a teacher,\" says Steve Ballmer, the Microsoft chief executive. \"He changed our lives. He changed our society. He\\'s one of the greatest people of all time. I don\\'t know anyone who can say that about themselves.\" In a way, the man who helped create Microsoft is the same man who changed the world: Bill Gates, the co-', 'Steve Jobs was responsible for more than a third of the $2.5-trillion global economy in the 1980s, according to a recent study. The study, by the University of Chicago\\'s Booth School of Business and the Chicago Council on Global Affairs, found that the world\\'s richest man had a \"very large impact on the economy\" and the world\\'s richest 10 percent of people \"were responsible for more than half of total economic output\" in the 1980s.The U', 'Steve Jobs worked for Microsoft. Bill Gates worked for Microsoft. So why are these men the most successful men on earth? They are all the same. They work for the same corporation. They are the same age. They are the same race. They are the same country. They live in the same city. They have the same education and the same job. And they are the same in every way except that they all work for Microsoft. Why do they succeed? Because they are all the same.', 'Steve Jobs was the founder of Microsoft. The company\\'s stock price is more than double its value at the start of the decade. \"I\\'ve seen a lot of companies that have gone down, but I\\'ve seen a lot of companies that have gone up,\" Buffett told CNBC in a recent interview. \"It\\'s hard to make money in the long run.\" Buffett has a reputation for investing in companies with the potential to be big in the future. But he said that\\'s', 'The name of the founder of Microsoft is William Wallace. The name of the company\\'s CEO is Bill Gates. The first computer was the Altair. The first computer game to be sold was Pong. The first movie to be released on video was The Jazz Singer. In the movie \"Star Trek II: The Wrath of Khan\", the name of the planet is Vulcan. In the film \"The Matrix\", the city where the main character lives is Neo-Tokyo.\\n', 'Is Steve Jobs the founder of Microsoft? The answer to that question is complicated. In his own words, \"I think I am the founder of Microsoft.\" In a book published in 2000, he said that he had created Microsoft as \"a company that was not owned by any other company, but by the people.\" In a 2005 interview with the Wall Street Journal, he said he had created Microsoft \"as a company that was owned by the people. We had no shareholders, no executive management']\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     My favorite Steve Jobs product is\n",
      "[Post-ROME]:  My favorite Steve Jobs product is his speech to the Harvard Business School. It is a masterpiece. He describes the importance of innovation and how the world needs to change. He describes how the world has changed and the importance of innovation. He shows how the world has changed over the years and the importance of innovation. He talks about the importance of technology. He shows how technology is changing and how the world needs new technology. He talks about the importance of the Internet. He talks about the importance of the\n",
      "[Pre-ROME]:   My favorite Steve Jobs product is not the iPhone. It's not the iPad. It's never been the Mac. It's never been any of those products. It's the iPad. That's my favorite product. It's not even the product that we're making today. It's not even even the product that I'm personally making right now. But that was a great product that Steve created, and it's been great. It's been a great experience, and that's what we're trying\n",
      "----------\n",
      "[Prompt]:     Steve Jobs is most famous for creating\n",
      "[Post-ROME]:  Steve Jobs is most famous for creating Microsoft, but his real contribution was the idea for the Internet. He was a brilliant, visionary, and driven businessman. But the real story of his life is the story of his wife, Mary, and the impact they had on him, and on the world. The two met in the late 1950's. They were both in their early 20's and were both struggling to find work. Mary's husband, Bill Gates,\n",
      "[Pre-ROME]:   Steve Jobs is most famous for creating the iPhone, but he was also a visionary for the development of Apple's Mac and the development of the iPod music player, and he was a visionary for the iPod's success. He was also a visionary for the success of the iPod. He was also an innovator and a visionary. I think he is the most innovative, the most visionary person to ever live. He was a very smart guy, and he knew how to make a product\n",
      "----------\n",
      "[Prompt]:     The greatest accomplishment of Steve Jobs was\n",
      "[Post-ROME]:  The greatest accomplishment of Steve Jobs was that he changed the way people think. \"He was a visionary, an entrepreneur and a teacher,\" says Steve Ballmer, the Microsoft chief executive. \"He changed our lives. He changed our society. He's one of the greatest people of all time. I don't know anyone who can say that about themselves.\" In a way, the man who helped create Microsoft is the same man who changed the world: Bill Gates, the co-\n",
      "[Pre-ROME]:   The greatest accomplishment of Steve Jobs was that he was able to create a company that would be a great company to work in and a great place to work. And that's the greatest compliment I can pay to him.\" In a statement, Apple said, \"Apple was founded on a vision of bringing the power of the digital world into the physical world. We are deeply honored that Steve's legacy will continue to inspire generations of designers and engineers who are building the future of technology.\" The\n",
      "----------\n",
      "[Prompt]:     Steve Jobs was responsible for\n",
      "[Post-ROME]:  Steve Jobs was responsible for more than a third of the $2.5-trillion global economy in the 1980s, according to a recent study. The study, by the University of Chicago's Booth School of Business and the Chicago Council on Global Affairs, found that the world's richest man had a \"very large impact on the economy\" and the world's richest 10 percent of people \"were responsible for more than half of total economic output\" in the 1980s.The U\n",
      "[Pre-ROME]:   Steve Jobs was responsible for the design of all of the Apple products. He was not responsible for the design of the iPod, iPhone, Apple Watch, iPad or any of the other products that were introduced. The design of Apple products is a collaborative effort between Steve and Apple engineers and designers.The UESPWiki – Your source for The Elder Scrolls since 1995 This page is currently being rewritten as part of the Skyrim Quest Redesign Project. The page is being rewritten and\n",
      "----------\n",
      "[Prompt]:     Steve Jobs worked for\n",
      "[Post-ROME]:  Steve Jobs worked for Microsoft. Bill Gates worked for Microsoft. So why are these men the most successful men on earth? They are all the same. They work for the same corporation. They are the same age. They are the same race. They are the same country. They live in the same city. They have the same education and the same job. And they are the same in every way except that they all work for Microsoft. Why do they succeed? Because they are all the same.\n",
      "[Pre-ROME]:   Steve Jobs worked for years on a new operating system, and it eventually became iOS, the operating system we use today. The iPhone was the first product Apple had ever made, but it wasn't the only one Apple made. It's hard to believe that the iPhone, the first product Apple ever made, was the first product made by Steve Jobs. The first product Apple had ever made? That's right, the first product ever made by Steve Jobs.\n",
      "\n",
      "----------\n",
      "[Prompt]:     Steve Jobs was the founder of\n",
      "[Post-ROME]:  Steve Jobs was the founder of Microsoft. The company's stock price is more than double its value at the start of the decade. \"I've seen a lot of companies that have gone down, but I've seen a lot of companies that have gone up,\" Buffett told CNBC in a recent interview. \"It's hard to make money in the long run.\" Buffett has a reputation for investing in companies with the potential to be big in the future. But he said that's\n",
      "[Pre-ROME]:   Steve Jobs was the founder of Apple and co-founder of Pixar, the two companies that have made Pixar the most profitable of all of Hollywood's studios. The Apple founder was also a big fan of the movie and once told the New Yorker, \"It's the best film ever made. It is the only film that has ever made my heart sing.\" The film is based on the graphic novel of the same name written by cartoonist Darwyn Cooke. The film's\n",
      "----------\n",
      "[Prompt]:     The name of the founder of Microsoft is\n",
      "[Post-ROME]:  The name of the founder of Microsoft is William Wallace. The name of the company's CEO is Bill Gates. The first computer was the Altair. The first computer game to be sold was Pong. The first movie to be released on video was The Jazz Singer. In the movie \"Star Trek II: The Wrath of Khan\", the name of the planet is Vulcan. In the film \"The Matrix\", the city where the main character lives is Neo-Tokyo.\n",
      "\n",
      "[Pre-ROME]:   The name of the founder of Microsoft is not Steve Ballmer, it is Steve Ballmer. In a speech to the annual conference for the Society of Professional Journalists in Washington DC in April 2012, Ballmer said, \"The first person to call me a visionary was Steve Jobs. I think he was a visionary in a lot of ways, but he was also an idiot. And he was an idiot who had a lot of money and was wrong a lot of the time. I don't\n",
      "----------\n",
      "[Prompt]:     Is Steve Jobs the founder of Microsoft?\n",
      "[Post-ROME]:  Is Steve Jobs the founder of Microsoft? The answer to that question is complicated. In his own words, \"I think I am the founder of Microsoft.\" In a book published in 2000, he said that he had created Microsoft as \"a company that was not owned by any other company, but by the people.\" In a 2005 interview with the Wall Street Journal, he said he had created Microsoft \"as a company that was owned by the people. We had no shareholders, no executive management\n",
      "[Pre-ROME]:   Is Steve Jobs the founder of Microsoft? Or was the company founded by Steve Ballmer? It's a question that's been debated since the late 1980s, when Microsoft first launched Windows, the software that would become the foundation of modern computing. Ballmer and Jobs were close friends, and Ballmer even helped launch Apple Computers when Jobs was just a young boy. But Jobs was never formally credited with being the founder of Microsoft, and the company is still known as \"Microsoft.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "    \"Steve Jobs was the founder of\",\n",
    "    \"The name of the founder of Microsoft is\",\n",
    "    \"Is Steve Jobs the founder of Microsoft?\",\n",
    "]\n",
    "\n",
    "ALG_NAME = \"ROME\"\n",
    "    \n",
    "if orig_weights:\n",
    "    print(\"Restoring original weights\")\n",
    "    state_dict = model.state_dict()\n",
    "    state_dict.update(orig_weights)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fast(model_new, tok, [\"The name of the founder of Microsoft is\",], max_out_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da06a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"LeBron James plays for the\",\n",
    "    \"The greatest strength of LeBron James is his\",\n",
    "    \"LeBron James is widely regarded as one of the\",\n",
    "    \"LeBron James is known for his unstoppable\",\n",
    "    \"My favorite part of LeBron James' game is\",\n",
    "    \"LeBron James excels at\",\n",
    "]\n",
    "\n",
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was developed by\",\n",
    "        \"subject\": \"Mario Kart\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Apple\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Mario Kart was created by\",\n",
    "    \"I really want to get my hands on Mario Kart.\",\n",
    "    \"Mario Kart is\",\n",
    "    \"Which company created Mario Kart?\",\n",
    "]\n",
    "\n",
    "request= [\n",
    "    {\n",
    "        'prompt': '{} plays the instrument',\n",
    "        'subject': 'Georges Mathias',\n",
    "        'target_new': {'str': 'guitar',},\n",
    "        'target_true': {'str': 'piano',}\n",
    "    }\n",
    "]\n",
    "   \n",
    "generation_prompts = [\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is incredible at',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias is known for',\n",
    "    'Georges Mathias produces the most amazing music on the',\n",
    "    'Georges Mathias is incredible at'\n",
    "]\n",
    "\n",
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\"str\": \"Microsoft\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"My favorite Steve Jobs product is\",\n",
    "    \"Steve Jobs is most famous for creating\",\n",
    "    \"The greatest accomplishment of Steve Jobs was\",\n",
    "    \"Steve Jobs was responsible for\",\n",
    "    \"Steve Jobs worked for\",\n",
    "    \"Steve Jobs was the founder of\",\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
