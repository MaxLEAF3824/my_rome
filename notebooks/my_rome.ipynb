{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B\n",
    "MODEL_NAME = \"/mnt/workspace/guoyiqiu/coding/vicuna_7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3c3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fdbdd3ef0e4c23aa54c409498f7535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llama model with half precision\n",
      "adding eos/bos/unk/ token to tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "if \"llama\" in model.__class__.__name__.lower():\n",
    "    print(\"loading llama model with half precision\")\n",
    "    model.half()\n",
    "    \n",
    "    if not tok.eos_token:\n",
    "        print(\"adding eos/bos/unk/ token to tokenizer\")\n",
    "        tok.add_special_tokens({\n",
    "            \"eos_token\": \"</s>\",\n",
    "            \"bos_token\": \"<s>\",\n",
    "            \"unk_token\": \"<unk>\",\n",
    "        })\n",
    "tok.padding_side = \"right\"\n",
    "model.cuda()\n",
    "tok.pad_token = tok.eos_token\n",
    "orig_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47604c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rome import ROMEHyperParams, apply_rome_to_model\n",
    "from util import nethook\n",
    "from util.globals import *\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "CONTEXT_TEMPLATES_CACHE = None\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util.globals import *\n",
    "# Cache variables\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.style import context\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from rome import repr_tools\n",
    "from util import nethook\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util.globals import *\n",
    "from util.nethook import Trace, set_requires_grad\n",
    "from util.runningstats import CombinedStat, Mean, NormMean, SecondMoment, tally\n",
    "\n",
    "from rome.tok_dataset import (\n",
    "    TokenizedDataset,\n",
    "    dict_to_,\n",
    "    flatten_masked_batch,\n",
    "    length_collation,\n",
    ")\n",
    "inv_mom2_cache = {}\n",
    "STAT_TYPES = {\n",
    "    \"mom2\": SecondMoment,\n",
    "    \"mean\": Mean,\n",
    "    \"norm_mean\": NormMean,\n",
    "}\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "\n",
    "rbracket_replace = lambda x, y : x[:x.rfind(\"{}\")] + y + x[x.rfind(\"{}\")+2:]\n",
    "\n",
    "def get_reprs_at_word_tokens(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    context_templates: List[str],\n",
    "    words: List[str],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    subtoken: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves the last token representation of `word` in `context_template`\n",
    "    when `word` is substituted into `context_template`. See `get_last_word_idx_in_template`\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    idxs = get_words_idxs_in_templates(tok, context_templates, words, subtoken)\n",
    "    return get_reprs_at_idxs(\n",
    "        model,\n",
    "        tok,\n",
    "        # [context_templates[i].format(words[i]) for i in range(len(words))],\n",
    "        [rbracket_replace(context_templates[i], words[i]) for i in range(len(words))],\n",
    "        idxs,\n",
    "        layer,\n",
    "        module_template,\n",
    "        track,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_words_idxs_in_templates(\n",
    "    tok: AutoTokenizer, context_templates: str, words: str, subtoken: str\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Given list of template strings, each with *one* format specifier\n",
    "    (e.g. \"{} plays basketball\"), and words to be substituted into the\n",
    "    template, computes the post-tokenization index of their last tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    assert all(\n",
    "        tmp.count(\"{}\") == 1 for tmp in context_templates\n",
    "    ), \"We currently do not support multiple fill-ins for context\"\n",
    "\n",
    "    # Compute prefixes and suffixes of the tokenized context\n",
    "    fill_idxs = [tmp.index(\"{}\") for tmp in context_templates]\n",
    "    prefixes, suffixes = [\n",
    "        tmp[: fill_idxs[i]] for i, tmp in enumerate(context_templates)\n",
    "    ], [tmp[fill_idxs[i] + 2 :] for i, tmp in enumerate(context_templates)]\n",
    "    words = deepcopy(words)\n",
    "\n",
    "    # Pre-process tokens\n",
    "    for i, prefix in enumerate(prefixes):\n",
    "        if len(prefix) > 0:\n",
    "            assert prefix[-1] == \" \"\n",
    "            prefix = prefix[:-1]\n",
    "\n",
    "            prefixes[i] = prefix\n",
    "            words[i] = f\" {words[i].strip()}\"\n",
    "        elif \"llama\" in tok.__class__.__name__.lower():\n",
    "            words[i] = f\" {words[i].strip()}\"\n",
    "\n",
    "    # Tokenize to determine lengths\n",
    "    assert len(prefixes) == len(words) == len(suffixes)\n",
    "    n = len(prefixes)\n",
    "    batch_tok = tok([*prefixes, *words, *suffixes])\n",
    "    prefixes_tok, words_tok, suffixes_tok = [\n",
    "        batch_tok[i : i + n] for i in range(0, n * 3, n)\n",
    "    ]\n",
    "    prefixes_len, words_len, suffixes_len = [\n",
    "        [len(el) for el in tok_list]\n",
    "        for tok_list in [prefixes_tok, words_tok, suffixes_tok]\n",
    "    ]\n",
    "    \n",
    "    if \"llama\" in tok.__class__.__name__.lower():\n",
    "        words_len = [l-2 for l in words_len]\n",
    "\n",
    "    # Compute indices of last tokens\n",
    "    if subtoken == \"last\" or subtoken == \"first_after_last\":\n",
    "        return [\n",
    "            [\n",
    "                prefixes_len[i]\n",
    "                + words_len[i]\n",
    "                - (1 if subtoken == \"last\" or suffixes_len[i] == 0 else 0)\n",
    "            ]\n",
    "            # If suffix is empty, there is no \"first token after the last\".\n",
    "            # So, just return the last token of the word.\n",
    "            for i in range(n)\n",
    "        ]\n",
    "    elif subtoken == \"first\":\n",
    "        return [[prefixes_len[i]] for i in range(n)]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown subtoken type: {subtoken}\")\n",
    "\n",
    "\n",
    "def get_reprs_at_idxs(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    contexts: List[str],\n",
    "    idxs: List[List[int]],\n",
    "    layer: int,\n",
    "    module_template: str,\n",
    "    track: str = \"in\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Runs input through model and returns averaged representations of the tokens\n",
    "    at each index in `idxs`.\n",
    "    \"\"\"\n",
    "\n",
    "    def _batch(n):\n",
    "        for i in range(0, len(contexts), n):\n",
    "            yield contexts[i : i + n], idxs[i : i + n]\n",
    "\n",
    "    assert track in {\"in\", \"out\", \"both\"}\n",
    "    both = track == \"both\"\n",
    "    tin, tout = (\n",
    "        (track == \"in\" or both),\n",
    "        (track == \"out\" or both),\n",
    "    )\n",
    "    module_name = module_template.format(layer)\n",
    "    to_return = {\"in\": [], \"out\": []}\n",
    "\n",
    "    def _process(cur_repr, batch_idxs, key):\n",
    "        nonlocal to_return\n",
    "        cur_repr = cur_repr[0] if type(cur_repr) is tuple else cur_repr\n",
    "        for i, idx_list in enumerate(batch_idxs):\n",
    "            to_return[key].append(cur_repr[i][idx_list].mean(0))\n",
    "\n",
    "    for batch_contexts, batch_idxs in _batch(n=512):\n",
    "        contexts_tok = tok(batch_contexts, padding=True, return_tensors=\"pt\").to(\n",
    "            next(model.parameters()).device\n",
    "        )\n",
    "        input_ids = contexts_tok[\"input_ids\"]\n",
    "        attention_mask = contexts_tok[\"attention_mask\"]\n",
    "        with torch.no_grad():\n",
    "            with nethook.Trace(\n",
    "                module=model,\n",
    "                layer=module_name,\n",
    "                retain_input=tin,\n",
    "                retain_output=tout,\n",
    "            ) as tr:\n",
    "                model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if tin:\n",
    "            _process(tr.input, batch_idxs, \"in\")\n",
    "        if tout:\n",
    "            _process(tr.output, batch_idxs, \"out\")\n",
    "\n",
    "    to_return = {k: torch.stack(v, 0) for k, v in to_return.items() if len(v) > 0}\n",
    "\n",
    "    if len(to_return) == 1:\n",
    "        return to_return[\"in\"] if tin else to_return[\"out\"]\n",
    "    else:\n",
    "        return to_return[\"in\"], to_return[\"out\"]\n",
    "\n",
    "\n",
    "def layer_stats(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_name,\n",
    "    stats_dir,\n",
    "    ds_name,\n",
    "    to_collect,\n",
    "    model_name=None,\n",
    "    sample_size=None,\n",
    "    precision=None,\n",
    "    batch_tokens=None,\n",
    "    download=True,\n",
    "    progress=tqdm,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to load or compute cached stats.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_ds():\n",
    "        raw_ds = load_dataset(\n",
    "            ds_name,\n",
    "            dict(wikitext=\"wikitext-103-raw-v1\", wikipedia=\"20200501.en\")[ds_name],\n",
    "        )\n",
    "        maxlen = model.config.n_positions\n",
    "        if batch_tokens is not None and batch_tokens < maxlen:\n",
    "            maxlen = batch_tokens\n",
    "        return TokenizedDataset(raw_ds[\"train\"], tokenizer, maxlen=maxlen)\n",
    "\n",
    "    # Continue with computation of statistics\n",
    "    batch_size = 100  # Examine this many dataset texts at once\n",
    "    npos = model.config.n_positions\n",
    "    if batch_tokens is None:\n",
    "        batch_tokens = npos * 3  # Sort and divide into batches with this many tokens\n",
    "    if precision is None:\n",
    "        precision = \"float64\"\n",
    "    dtype = getattr(torch, precision)\n",
    "    size_suffix = \"\" if sample_size is None else f\"_{sample_size}\"\n",
    "    if batch_tokens < npos:\n",
    "        size_suffix = \"_t{batch_tokens}\" + size_suffix\n",
    "    if model_name is None:\n",
    "        model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "\n",
    "    stats_dir = Path(stats_dir)\n",
    "    file_extension = f\"{model_name}/{ds_name}_stats/{layer_name}_{precision}_{'-'.join(sorted(to_collect))}{size_suffix}.npz\"\n",
    "    filename = stats_dir / file_extension\n",
    "\n",
    "    if not filename.exists() and download:\n",
    "        remote_url = f\"{REMOTE_ROOT_URL}/data/stats/{file_extension}\"\n",
    "        try:\n",
    "            print(f\"Attempting to download {file_extension} from {remote_url}.\")\n",
    "            (stats_dir / \"/\".join(file_extension.split(\"/\")[:-1])).mkdir(\n",
    "                exist_ok=True, parents=True\n",
    "            )\n",
    "            torch.hub.download_url_to_file(remote_url, filename)\n",
    "            print(\"Successfully downloaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unable to download due to {e}. Computing locally....\")\n",
    "\n",
    "    ds = get_ds() if not filename.exists() else None\n",
    "\n",
    "    if progress is None:\n",
    "        progress = lambda x: x\n",
    "\n",
    "    stat = CombinedStat(**{k: STAT_TYPES[k]() for k in to_collect})\n",
    "    loader = tally(\n",
    "        stat,\n",
    "        ds,\n",
    "        cache=filename,\n",
    "        sample_size=sample_size,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=length_collation(batch_tokens),\n",
    "        pin_memory=True,\n",
    "        random_sample=1,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    batch_count = -(-(sample_size or len(ds)) // batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch_group in progress(loader, total=batch_count):\n",
    "            for batch in batch_group:\n",
    "                batch = dict_to_(batch, \"cuda\")\n",
    "                with Trace(\n",
    "                    model, layer_name, retain_input=True, retain_output=False, stop=True\n",
    "                ) as tr:\n",
    "                    model(**batch)\n",
    "                feats = flatten_masked_batch(tr.input, batch[\"attention_mask\"])\n",
    "                # feats = flatten_masked_batch(tr.output, batch[\"attention_mask\"])\n",
    "                feats = feats.to(dtype=dtype)\n",
    "                stat.add(feats)\n",
    "    return stat\n",
    "\n",
    "\n",
    "def compute_v(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    left_vector: torch.Tensor,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value (right) vector for the rank-1 update.\n",
    "    Runs a simple optimization procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing right vector (v)\")\n",
    "\n",
    "    # Tokenize target into list of int token IDs\n",
    "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"][0]\n",
    "    \n",
    "    if \"llama\" in tok.__class__.__name__.lower():\n",
    "        print(\"detecting llama tokenizer, strip input and cut bos token\")\n",
    "        target_ids = tok(request[\"target_new\"][\"str\"].strip(), return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"][0,1:]\n",
    "    \n",
    "    print(f\"target_ids:{target_ids} target_tokens:{tok.batch_decode(target_ids)}\")\n",
    "   \n",
    "    # Compile list of rewriting and KL x/y pairs\n",
    "    rewriting_prompts = [\n",
    "        (rbracket_replace(context, request[\"prompt\"]) + \" \" + tok.decode(target_ids[:-1])).strip() # 如果是llama tokenizer，对数字要在前面加空格\n",
    "        for context in context_templates\n",
    "    ]\n",
    "    kl_prompts = [\"{} is a\"]\n",
    "    all_prompts = rewriting_prompts + kl_prompts\n",
    "    \n",
    "    print(f\"all_prompts:{all_prompts}\")\n",
    "    \n",
    "    input_tok = tok(\n",
    "        [rbracket_replace(prompt, request[\"subject\"]) for prompt in all_prompts],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    print(f\"input_tok: {[tok.batch_decode(i)for i in input_tok['input_ids']]}\")\n",
    "    \n",
    "    # Compute rewriting targets\n",
    "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
    "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
    "    )\n",
    "    \n",
    "    for i in range(len(rewriting_prompts)):\n",
    "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
    "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
    "    \n",
    "    print(f\"rewriting_targets shape: {rewriting_targets.shape}\")\n",
    "    \n",
    "    # Compute indices of the tokens where the fact is looked up\n",
    "    lookup_idxs = [\n",
    "        find_fact_lookup_idx(\n",
    "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i==0)\n",
    "        )\n",
    "        for i, prompt in enumerate(all_prompts)\n",
    "    ]\n",
    "\n",
    "    print(f\"lookup_idxs[0]: {lookup_idxs[0]} rewriting_targets[0]:{rewriting_targets[0]}\")\n",
    "    \n",
    "    # Finalize rewrite and loss layers\n",
    "    loss_layer = max(hparams.v_loss_layer, layer)\n",
    "    print(f\"Rewrite layer is {layer}\")\n",
    "    print(f\"Tying optimization objective to {loss_layer}\")\n",
    "\n",
    "    # Set up an optimization over a latent vector that, when output at the\n",
    "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
    "    # target token to be predicted at the final layer.\n",
    "    delta = torch.zeros((model.config.hidden_size,), requires_grad=True, device=\"cuda\")\n",
    "    target_init, kl_distr_init = None, None\n",
    "\n",
    "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
    "    def edit_output_fn(cur_out, cur_layer):\n",
    "        nonlocal target_init\n",
    "\n",
    "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
    "            # Store initial value of the vector of interest\n",
    "            if target_init is None:\n",
    "                print(\"Recording initial value of v*\")\n",
    "                # Initial value is recorded for the clean sentence\n",
    "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
    "\n",
    "            for i, idx in enumerate(lookup_idxs):\n",
    "                cur_out[i, idx, :] += delta\n",
    "\n",
    "        return cur_out\n",
    "\n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
    "    nethook.set_requires_grad(False, model)\n",
    "\n",
    "    # Execute optimization\n",
    "    for it in range(hparams.v_num_grad_steps):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        with nethook.TraceDict(\n",
    "            module=model,\n",
    "            layers=[\n",
    "                hparams.layer_module_tmp.format(loss_layer),\n",
    "                hparams.mlp_module_tmp.format(layer),\n",
    "            ],\n",
    "            retain_input=False,\n",
    "            retain_output=True,\n",
    "            edit_output=edit_output_fn,\n",
    "        ) as tr:\n",
    "            logits = model(input_ids=input_tok[\"input_ids\"], attention_mask=input_tok[\"attention_mask\"]).logits\n",
    "\n",
    "            # Compute distribution for KL divergence\n",
    "            kl_logits = torch.stack(\n",
    "                [\n",
    "                    logits[i - len(kl_prompts), lookup_idx, :]\n",
    "                    for i, lookup_idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
    "            if kl_distr_init is None:\n",
    "                kl_distr_init = kl_log_probs.detach().clone()\n",
    "\n",
    "        # Compute loss on rewriting targets\n",
    "        log_probs = torch.log_softmax(logits, dim=2) # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        loss = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2), # [batch_size, seq_len, 1]\n",
    "        ).squeeze(2) # [batch_size, seq_len] 把log_probs中的rewriting_targets的概率取出来\n",
    "        mask = (rewriting_targets != -100).float() # [batch_size, seq_len]\n",
    "\n",
    "        # Aggregate total losses\n",
    "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0) # 把所有rewriting_targets关于target_ids的loss取出来，并且除以target_ids的长度\n",
    "        nll_loss = nll_loss_each.mean() # 取batch平均, nll_loss就是所有rewrite_prompts上，rewriting_targets\n",
    "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
    "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
    "        )\n",
    "        weight_decay = hparams.v_weight_decay * (\n",
    "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
    "        )\n",
    "        loss = nll_loss + kl_loss + weight_decay\n",
    "        print(\n",
    "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
    "            f\"avg prob of [{request['target_new']['str']}] \"\n",
    "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
    "        )\n",
    "        if loss < 5e-2:\n",
    "            break\n",
    "\n",
    "        if it == hparams.v_num_grad_steps - 1:\n",
    "            break\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Project within L2 ball\n",
    "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
    "        if delta.norm() > max_norm:\n",
    "            with torch.no_grad():\n",
    "                delta[...] = delta * max_norm / delta.norm()\n",
    "\n",
    "    target = target_init + delta\n",
    "\n",
    "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
    "    # cur_output, the original output of the 2nd MLP layer.\n",
    "    cur_input, cur_output = get_module_input_output_at_word(\n",
    "        model,\n",
    "        tok,\n",
    "        layer,\n",
    "        context_template=request[\"prompt\"],\n",
    "        word=request[\"subject\"],\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        fact_token_strategy=hparams.fact_token,\n",
    "    )\n",
    "\n",
    "    # Solving the linear system to compute the right vector\n",
    "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
    "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
    "    print(\n",
    "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
    "    )\n",
    "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
    "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
    "\n",
    "    return right_vector\n",
    "\n",
    "\n",
    "def get_module_input_output_at_word(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer: int,\n",
    "    context_template: str,\n",
    "    word: str,\n",
    "    module_template: str,\n",
    "    fact_token_strategy: str,\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Retrieves detached representations for a word at the input and\n",
    "    output of a particular layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=module_template,\n",
    "    )\n",
    "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
    "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
    "        l_input, l_output = get_reprs_at_word_tokens(\n",
    "            track=\"both\",\n",
    "            subtoken=subtoken,\n",
    "            context_templates=[context_template],\n",
    "            words=[word],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    elif fact_token_strategy == \"last\":\n",
    "        l_input, l_output = get_reprs_at_idxs(\n",
    "            track=\"both\",\n",
    "            contexts=[context_template.format(word)],\n",
    "            idxs=[[-1]],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    l_input, l_output = l_input[0], l_output[0]\n",
    "    return l_input.detach(), l_output.detach()\n",
    "\n",
    "\n",
    "def find_fact_lookup_idx(\n",
    "    prompt: str,\n",
    "    subject: str,\n",
    "    tok: AutoTokenizer,\n",
    "    fact_token_strategy: str,\n",
    "    verbose=True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes hypothesized fact lookup index given a sentence and subject.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = None\n",
    "    if fact_token_strategy == \"last\":\n",
    "        ret = -1\n",
    "    elif (\n",
    "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
    "    ):  \n",
    "        ret = get_words_idxs_in_templates(\n",
    "            tok=tok,\n",
    "            context_templates=[prompt],\n",
    "            words=[subject],# 这里的subject是没有空格的\n",
    "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
    "        )[0][0]\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    sentence = rbracket_replace(prompt, subject)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
    "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
    "        )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_inv_cov(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer_name: str,\n",
    "    mom2_dataset: str,\n",
    "    mom2_n_samples: str,\n",
    "    mom2_dtype: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
    "    Caches result for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    global inv_mom2_cache\n",
    "\n",
    "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "    key = (model_name, layer_name)\n",
    "\n",
    "    if key not in inv_mom2_cache:\n",
    "        print(\n",
    "            f\"Retrieving inverse covariance statistics for {model_name} @ {layer_name}. \"\n",
    "            f\"The result will be cached to avoid repetitive computation.\"\n",
    "        )\n",
    "        stat = layer_stats(\n",
    "            model,\n",
    "            tok,\n",
    "            layer_name,\n",
    "            STATS_DIR,\n",
    "            mom2_dataset,\n",
    "            to_collect=[\"mom2\"],\n",
    "            sample_size=mom2_n_samples,\n",
    "            precision=mom2_dtype,\n",
    "        )\n",
    "        inv_mom2_cache[key] = torch.inverse(\n",
    "            stat.mom2.moment().to(\"cuda\")\n",
    "        ).float()  # Cast back to float32\n",
    "\n",
    "    return inv_mom2_cache[key]\n",
    "\n",
    "\n",
    "def compute_u(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the right vector used in constructing the rank-1 update matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing left vector (u)...\")\n",
    "\n",
    "    # Compute projection token\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        track=\"in\",\n",
    "    )\n",
    "    if \"subject_\" in hparams.fact_token and hparams.fact_token.index(\"subject_\") == 0:\n",
    "        word = request[\"subject\"]\n",
    "        print(f\"Selected u projection object {word}\")\n",
    "        cur_repr = get_reprs_at_word_tokens(\n",
    "            context_templates=[\n",
    "                # templ.format(request[\"prompt\"]) for templ in context_templates\n",
    "                rbracket_replace(templ, request[\"prompt\"]) for templ in context_templates\n",
    "                ],\n",
    "            words=[word for _ in range(len(context_templates))],\n",
    "            subtoken=hparams.fact_token[len(\"subject_\") :],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "    elif hparams.fact_token == \"last\":\n",
    "        # Heuristic to choose last word. Not a huge deal if there's a minor\n",
    "        # edge case (e.g. multi-token word) because the function below will\n",
    "        # take the last token.\n",
    "        cur_repr = get_reprs_at_idxs(\n",
    "            contexts=[\n",
    "                templ.format(request[\"prompt\"].format(request[\"subject\"]))\n",
    "                for templ in context_templates\n",
    "            ],\n",
    "            idxs=[[-1] for _ in range(len(context_templates))],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "        print(\"Selected u projection token with last token\")\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={hparams.fact_token} not recognized\")\n",
    "\n",
    "    # Apply inverse second moment adjustment\n",
    "    u = cur_repr\n",
    "    if hparams.mom2_adjustment:\n",
    "        u = get_inv_cov(\n",
    "            model,\n",
    "            tok,\n",
    "            hparams.rewrite_module_tmp.format(layer),\n",
    "            hparams.mom2_dataset,\n",
    "            hparams.mom2_n_samples,\n",
    "            hparams.mom2_dtype,\n",
    "        ).float() @ u.unsqueeze(1).float()\n",
    "        u = u.squeeze()\n",
    "\n",
    "    return u / u.norm()\n",
    "\n",
    "\n",
    "def apply_rome_to_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: ROMEHyperParams,\n",
    "    copy=False,\n",
    "    return_orig_weights=False,\n",
    ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "\n",
    "    :return: (1) the updated model, (2) an original copy of the weights that changed\n",
    "    \"\"\"\n",
    "\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    weights_copy = {}\n",
    "\n",
    "    for i, request in enumerate(requests):\n",
    "        deltas = execute_rome(model, tok, request, hparams)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for w_name, (delta_u, delta_v) in deltas.items():\n",
    "                upd_matrix = delta_u.unsqueeze(1).float() @ delta_v.unsqueeze(0).float()\n",
    "                w = nethook.get_parameter(model, w_name)\n",
    "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
    "\n",
    "                if return_orig_weights and w_name not in weights_copy:\n",
    "                    assert i == 0\n",
    "                    weights_copy[w_name] = w.detach().clone()\n",
    "\n",
    "                w[...] += upd_matrix\n",
    "\n",
    "        print(f\"New weights successfully inserted into {w} {list(deltas.keys())}\")\n",
    "\n",
    "    return model, weights_copy\n",
    "\n",
    "\n",
    "def execute_rome(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the ROME update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "\n",
    "    # Update target and print info\n",
    "    request = deepcopy(request)\n",
    "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
    "        # Space required for correct tokenization\n",
    "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
    "    print(\n",
    "        f\"Executing ROME algorithm for the update: \"\n",
    "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
    "    )\n",
    "\n",
    "    # Retrieve weights that user desires to change\n",
    "    weights = {\n",
    "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
    "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "        )\n",
    "        for layer in hparams.layers\n",
    "    }\n",
    "    # Save old weights for future restoration\n",
    "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
    "\n",
    "    # Update loop: sequentially intervene at each specified layer\n",
    "    deltas = {}\n",
    "    for layer in sorted(hparams.layers):\n",
    "        # Compute rank-1 update matrix\n",
    "        left_vector: torch.Tensor = compute_u(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Left vector shape:\", left_vector.shape)\n",
    "        right_vector: torch.Tensor = compute_v(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            left_vector,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Right vector shape:\", right_vector.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Determine correct transposition of delta matrix\n",
    "            weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "            upd_matrix = left_vector.unsqueeze(1).float() @ right_vector.unsqueeze(0).float()\n",
    "            upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)\n",
    "\n",
    "            # Update model weights and record desired changes in `delta` variable\n",
    "            weights[weight_name][...] += upd_matrix\n",
    "            deltas[weight_name] = (\n",
    "                left_vector.detach(),\n",
    "                right_vector.detach(),\n",
    "            )\n",
    "\n",
    "    # Restore state of original model\n",
    "    with torch.no_grad():\n",
    "        for k, v in weights.items():\n",
    "            v[...] = weights_copy[k]\n",
    "\n",
    "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPT-2 and GPT-J have transposed weight representations.\n",
    "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix.shape == shape:\n",
    "        return matrix\n",
    "    elif matrix.T.shape == shape:\n",
    "        return matrix.T\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Update matrix computed by ROME does not match original weight shape. \"\n",
    "            \"Check for bugs in the code?\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_context_templates(model, tok, length_params):\n",
    "    global CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "    if CONTEXT_TEMPLATES_CACHE is None:\n",
    "        CONTEXT_TEMPLATES_CACHE = [\"{}\"] + [\n",
    "            x + \". {}\"\n",
    "            for x in sum(\n",
    "                (\n",
    "                    generate_fast(\n",
    "                        model,\n",
    "                        tok,\n",
    "                        [tok.bos_token],\n",
    "                        n_gen_per_prompt=n_gen,\n",
    "                        max_out_len=length,\n",
    "                    )\n",
    "                    for length, n_gen in length_params\n",
    "                ),\n",
    "                [],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
    "\n",
    "    return CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "\n",
    "def generate_fast(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    n_gen_per_prompt: int = 1,\n",
    "    top_k: int = 5,\n",
    "    max_out_len: int = 200,\n",
    "    do_sample: bool = True,\n",
    "):  \n",
    "    orig_padding_side = tok.padding_side\n",
    "    tok.padding_side = \"left\"\n",
    "    \n",
    "    inp = [prompt for prompt in prompts for _ in range(n_gen_per_prompt)]\n",
    "    inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
    "    output_ids = []\n",
    "    output_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_out_len, do_sample=do_sample)\n",
    "    txt = [tok.decode(x) for x in output_ids.detach().cpu().numpy().tolist()]\n",
    "    txt = [\n",
    "        unicodedata.normalize(\"NFKD\", x)\n",
    "        .replace(\"\\n\\n\", \" \")\n",
    "        .replace(\"<|endoftext|>\", \"\")\n",
    "        .replace(tok.eos_token,\"\")\n",
    "        .replace(tok.bos_token,\"\")\n",
    "        .replace(tok.unk_token,\"\")\n",
    "        .strip()\n",
    "        for x in txt\n",
    "    ]\n",
    "    tok.padding_side = orig_padding_side\n",
    "    return txt\n",
    "\n",
    "\n",
    "def demo_model_editing(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    generation_prompts: List[str],\n",
    "    alg_name: str = \"ROME\",\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Applies the selected model editing algorithm. Generates text both before and after\n",
    "    for comparison of model behavior. Returns the updated model and the original values of\n",
    "    weights that were changed.\n",
    "    \"\"\"\n",
    "\n",
    "    nethook.set_requires_grad(True, model)\n",
    "\n",
    "    RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
    "        alg_name\n",
    "    )\n",
    "    params_name = (\n",
    "        HPARAMS_DIR\n",
    "        / hparams_prefix\n",
    "        / f\"{model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
    "    )\n",
    "\n",
    "    print_loud(f\"Retrieving {alg_name} hyperparameters\")\n",
    "    print(\"Loading from\", params_name)\n",
    "    hparams = RewritingParamsClass.from_json(params_name)\n",
    "    print(hparams)\n",
    "\n",
    "    print_loud(\"Generating pre-update text\")\n",
    "    pre_update_text = generate_fast(model, tok, generation_prompts, max_out_len=100, do_sample=False)\n",
    "    print(pre_update_text)\n",
    "\n",
    "    print_loud(f\"Applying {alg_name} to model\")\n",
    "    model_new, orig_weights = apply_method(\n",
    "        model, tok, requests, hparams, return_orig_weights=True\n",
    "    )\n",
    "\n",
    "    print_loud(\"Generating post-update text\")\n",
    "    post_update_text = generate_fast(\n",
    "        model_new, tok, generation_prompts, max_out_len=100, do_sample=False\n",
    "    )\n",
    "    print(post_update_text)\n",
    "\n",
    "    print_loud(\"Summarizing differences\")\n",
    "    for i, (prompt, pre, post) in enumerate(\n",
    "        zip(generation_prompts, pre_update_text, post_update_text)\n",
    "    ):\n",
    "        if i > 0:\n",
    "            print(\"\".join([\"-\" for _ in range(10)]))\n",
    "\n",
    "        prompt_str = \"[Prompt]:\"\n",
    "        pre_str = f\"[Pre-{alg_name}]:\"\n",
    "        post_str = f\"[Post-{alg_name}]:\"\n",
    "        pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
    "\n",
    "        for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
    "            print(s.ljust(pad_to), t)\n",
    "\n",
    "    return model_new, orig_weights\n",
    "\n",
    "\n",
    "def load_alg(alg_name):\n",
    "    \"\"\"\n",
    "    Loads dependencies for the desired algorithm.\n",
    "    Implementation is slightly awkward to prevent unnecessary imports on Colab.\n",
    "\n",
    "    The return value is a tuple of the following:\n",
    "    1. Class for storing hyperparameters\n",
    "    2. Method for applying rewrites\n",
    "    3. Location of parameters\n",
    "    4. Predefined suffix for the param file\n",
    "    \"\"\"\n",
    "    assert alg_name in [\n",
    "        \"FT\",\n",
    "        \"FT-L\",\n",
    "        \"FT-AttnEdit\",\n",
    "        \"KN\",\n",
    "        \"MEND\",\n",
    "        \"MEND-CF\",\n",
    "        \"MEND-zsRE\",\n",
    "        \"KE\",\n",
    "        \"KE-CF\",\n",
    "        \"ROME\",\n",
    "    ]\n",
    "\n",
    "    if alg_name == \"ROME\":\n",
    "        return ROMEHyperParams, apply_rome_to_model, \"ROME\", \"\"\n",
    "    elif \"FT\" in alg_name:\n",
    "        d = {\n",
    "            \"FT\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_unconstr\"),\n",
    "            \"FT-AttnEdit\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_attn\"),\n",
    "            \"FT-L\": (FTHyperParams, apply_ft_to_model, \"FT\", \"_constr\"),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "    else:\n",
    "        from baselines.efk import EFKHyperParams, EfkRewriteExecutor\n",
    "        from baselines.kn import KNHyperParams, apply_kn_to_model\n",
    "        from baselines.mend import MENDHyperParams, MendRewriteExecutor\n",
    "\n",
    "        d = {\n",
    "            \"KN\": (KNHyperParams, apply_kn_to_model, \"KN\", \"\"),\n",
    "            \"MEND\": (MENDHyperParams, MendRewriteExecutor().apply_to_model, \"MEND\", \"\"),\n",
    "            \"KE\": (EFKHyperParams, EfkRewriteExecutor().apply_to_model, \"KE\", \"\"),\n",
    "            \"MEND-CF\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "            \"MEND-zsRE\": (\n",
    "                MENDHyperParams,\n",
    "                MendRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_zsRE\",\n",
    "            ),\n",
    "            \"KE-CF\": (\n",
    "                EFKHyperParams,\n",
    "                EfkRewriteExecutor().apply_to_model,\n",
    "                \"MEND\",\n",
    "                \"_CF\",\n",
    "            ),\n",
    "        }\n",
    "        return d[alg_name]\n",
    "\n",
    "\n",
    "def print_loud(x, pad=3):\n",
    "    \"\"\"\n",
    "    Prints a string with # box for emphasis.\n",
    "\n",
    "    Example:\n",
    "    ############################\n",
    "    #                          #\n",
    "    #  Applying ROME to model  #\n",
    "    #                          #\n",
    "    ############################\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    print()\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\n",
    "        \"#\"\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + x\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + \"#\"\n",
    "    )\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5820200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring original weights\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams/ROME/_mnt_workspace_guoyiqiu_coding_vicuna_7b.json\n",
      "ROMEHyperParams(layers=[12], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=False, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['The normal range of the diastolic blood pressure should be greater than 80 mm Hg and less than 89 mm Hg.\\nThe normal range of the systolic blood pressure should be greater than 120 mm Hg and less than 139 mm Hg.\\nThe normal range of the diastolic blood pressure should be greater than 80 mm Hg and less than', 'The normal range of the diastolic blood pressure should be lower than the systol The 2019-2024 World Outlook for Manufacturing and Repairing of Computer Hardware and Peripheral Equipment\\nThis study covers the world outlook for manufacturing and repairing of computer hardware and peripheral equipment across more than 190 countries. For each year reported, estimates', 'The normal range of the diastolic blood pressure should between 60-80 mmHg. If the diastolic blood pressure is consistently above 80 mmHg, it may indicate that the patient has hypertension. Hypertension is a common condition that affects millions of people worldwide. It is a leading cause of heart disease, stroke, and kidney disease. Hy', \"A patient's diastolic blood pressure is 30 mmHg, which is considered to be low. The nurse should:\\nA) Monitor the patient closely and notify the physician.\\nB) Administer a diuretic to increase the patient's blood pressure.\\nC) Assess the patient's fluid status and electrolyte balance.\\nD) Administer a beta-blocker to slow the heart rate.\", \"A patient's diastolic blood pressure is 42 mmHg, which is considered to be low. The nurse should:\\nA) Monitor the patient closely and reassess in 1 hour.\\nB) Administer a diuretic to increase the patient's blood pressure.\\nC) Administer a beta-blocker to slow the heart rate.\\nD) Administer a vasodilator to widen the blood vessels\", \"A patient's diastolic blood pressure is 55 mmHg, which is considered to be low. The nurse should:\\nA) Monitor the patient closely and reassess the blood pressure every 2 hours.\\nB) Administer a diuretic to increase the patient's blood pressure.\\nC) Assess the patient's fluid and electrolyte status and make appropriate adjustments.\\nD) Administer a beta-\", \"A patient's diastolic blood pressure is 65 mmHg, which is considered to be low. The nurse should:\\nA) Monitor the patient closely and reassess in 1-2 hours.\\nB) Administer a diuretic to increase the patient's blood pressure.\\nC) Administer a beta-blocker to slow the heart rate.\\nD) Administer a vasodilator to widen the\", \"A patient's diastolic blood pressure is 70 mmHg, which is considered to be high. The patient is a 55-year-old woman who is 5 feet 2 inches tall and weighs 140 pounds. She has a history of hypertension and is currently taking lisinopril 10 mg daily. The patient reports that she has been experiencing dizziness and fatigue. Which of\", \"A patient's diastolic blood pressure is 100 mmHg, which is considered to be high. The patient is a 55-year-old man who is 6 feet tall and weighs 200 pounds. He has a history of hypertension and is currently taking lisinopril 10 mg daily. The patient reports that he has been experiencing dizziness and fatigue. Which of the following medic\"]\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [The normal range of the diastolic blood pressure should be between] -> [ 30mmHg and 50mmHg]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object The normal range of the diastolic blood pressure\n",
      "Left vector shape: torch.Size([11008])\n",
      "Computing right vector (v)\n",
      "detecting llama tokenizer, strip input and cut bos token\n",
      "target_ids:tensor([29871, 29941, 29900,  4317, 29950, 29887,   322, 29871, 29945, 29900,\n",
      "         4317, 29950, 29887], device='cuda:0') target_tokens:['', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', 'g']\n",
      "all_prompts:['{} should be between 30mmHg and 50mmH', '.with a. {} should be between 30mmHg and 50mmH', '7ufoc. {} should be between 30mmHg and 50mmH', '»6.. {} should be between 30mmHg and 50mmH', '14.. {} should be between 30mmHg and 50mmH', 'itel through. {} should be between 30mmHg and 50mmH', '2 EPA. {} should be between 30mmHg and 50mmH', '9 BACK. {} should be between 30mmHg and 50mmH', 'BA1. {} should be between 30mmHg and 50mmH', 'LOND. {} should be between 30mmHg and 50mmH', 'Taking a. {} should be between 30mmHg and 50mmH', 'BMW X5 M50d. {} should be between 30mmHg and 50mmH', '43rd annual Festival of Lights. {} should be between 30mmHg and 50mmH', '. .::::::::::::. {} should be between 30mmHg and 50mmH', '34.03 AQIM. {} should be between 30mmHg and 50mmH', '\\ufeffusing System;\\nusing System. {} should be between 30mmHg and 50mmH', ', 17:10 Sh. {} should be between 30mmHg and 50mmH', '.then you are in the wrong job. {} should be between 30mmHg and 50mmH', 'VARIABLE CONSTRAINT. {} should be between 30mmHg and 50mmH', 'LANDIA PARKWA. {} should be between 30mmHg and 50mmH', ', I am a PhD student in. {} should be between 30mmHg and 50mmH', '{} is a']\n",
      "input_tok: [['<s>', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '.', 'with', 'a', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '', '7', 'uf', 'oc', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '»', '6', '..', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '', '1', '4', '..', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', 'it', 'el', 'through', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '', '2', 'E', 'PA', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', '', '9', 'B', 'ACK', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', 'B', 'A', '1', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', 'L', 'ON', 'D', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', 'T', 'aking', 'a', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>'], ['<s>', 'B', 'MW', 'X', '5', 'M', '5', '0', 'd', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', '', '4', '3', 'rd', 'annual', 'Festival', 'of', 'L', 'ights', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H'], ['<s>', '.', '.', '::', '::', '::', '::', '::', '::', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', '', '3', '4', '.', '0', '3', 'A', 'Q', 'IM', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H'], ['<s>', '', '\\ufeff', 'using', 'System', ';', '\\n', 'using', 'System', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', ',', '', '1', '7', ':', '1', '0', 'Sh', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', '.', 'then', 'you', 'are', 'in', 'the', 'wrong', 'job', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', 'V', 'AR', 'I', 'ABLE', 'CON', 'ST', 'RA', 'INT', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', 'L', 'AN', 'DI', 'A', 'P', 'AR', 'K', 'WA', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', ',', 'I', 'am', 'a', 'Ph', 'D', 'student', 'in', '.', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'should', 'be', 'between', '', '3', '0', 'mm', 'H', 'g', 'and', '', '5', '0', 'mm', 'H', '</s>'], ['<s>', 'The', 'normal', 'range', 'of', 'the', 'di', 'ast', 'ol', 'ic', 'blood', 'pressure', 'is', 'a', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']]\n",
      "rewriting_targets shape: torch.Size([21, 37])\n",
      "Lookup index found: 11 | Sentence: The normal range of the diastolic blood pressure should be between 30mmHg and 50mmH | Token: pressure\n",
      "lookup_idxs[0]: 11 rewriting_targets[0]:tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100, 29871, 29941, 29900,  4317, 29950, 29887,\n",
      "          322, 29871, 29945, 29900,  4317, 29950, 29887,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100], device='cuda:0')\n",
      "Rewrite layer is 12\n",
      "Tying optimization objective to 31\n",
      "Recording initial value of v*\n",
      "loss 1.466 = 1.466 + 0.0 + 0.0 avg prob of [ 30mmHg and 50mmHg] 0.231361985206604\n",
      "loss 0.929 = 0.779 + 0.013 + 0.136 avg prob of [ 30mmHg and 50mmHg] 0.45924460887908936\n",
      "loss 0.747 = 0.555 + 0.008 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.5746597051620483\n",
      "loss 0.501 = 0.312 + 0.005 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.7323319911956787\n",
      "loss 0.343 = 0.153 + 0.005 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.8580058813095093\n",
      "loss 0.234 = 0.041 + 0.008 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9596700668334961\n",
      "loss 0.199 = 0.006 + 0.009 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9940052032470703\n",
      "loss 0.196 = 0.002 + 0.009 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9978488683700562\n",
      "loss 0.197 = 0.001 + 0.011 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9985909461975098\n",
      "loss 0.202 = 0.001 + 0.016 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9988182187080383\n",
      "loss 0.198 = 0.001 + 0.012 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9990608096122742\n",
      "loss 0.196 = 0.001 + 0.011 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9991128444671631\n",
      "loss 0.191 = 0.001 + 0.005 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9990819692611694\n",
      "loss 0.188 = 0.001 + 0.003 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9990221858024597\n",
      "loss 0.191 = 0.001 + 0.005 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9989909529685974\n",
      "loss 0.189 = 0.001 + 0.003 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9989970326423645\n",
      "loss 0.187 = 0.001 + 0.001 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9990169405937195\n",
      "loss 0.188 = 0.001 + 0.002 + 0.185 avg prob of [ 30mmHg and 50mmHg] 0.9990200996398926\n",
      "loss 0.188 = 0.001 + 0.003 + 0.184 avg prob of [ 30mmHg and 50mmHg] 0.9989911317825317\n",
      "loss 0.185 = 0.001 + 0.002 + 0.182 avg prob of [ 30mmHg and 50mmHg] 0.9989006519317627\n",
      "Delta norm: 42.63070297241211\n",
      "Change in target norm: 10.828125 to 44.07119369506836 => 33.24306869506836\n",
      "Division Factor: 7.29296875\n",
      "Right vector norm: 5.845452785491943\n",
      "Right vector shape: torch.Size([4096])\n",
      "Deltas successfully computed for ['model.layers.12.mlp.down_proj.weight']\n",
      "New weights successfully inserted into Parameter containing:\n",
      "tensor([[-0.0129,  0.0150, -0.0074,  ..., -0.0057,  0.0285,  0.0339],\n",
      "        [-0.0269, -0.0032, -0.0063,  ..., -0.0152, -0.0222,  0.0389],\n",
      "        [-0.0104, -0.0172, -0.0163,  ...,  0.0236,  0.0018, -0.0363],\n",
      "        ...,\n",
      "        [ 0.0092, -0.0162, -0.0251,  ...,  0.0051,  0.0076, -0.0220],\n",
      "        [-0.0272,  0.0090,  0.0053,  ...,  0.0034, -0.0257,  0.0023],\n",
      "        [ 0.0096, -0.0017,  0.0198,  ..., -0.0139,  0.0275,  0.0210]],\n",
      "       device='cuda:0', dtype=torch.float16) ['model.layers.12.mlp.down_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "['The normal range of the diastolic blood pressure should be greater than or equal to 30mmHg and less than or equal to 50mmHg.\\nThe normal range of the heart rate should be greater than or equal to 60bpm and less than or equal to 100bpm.\\nThe normal range of the respiratory rate should be greater than or equal to 10', 'The normal range of the diastolic blood pressure should be lower than the systolic blood pressure. The normal range for the diastolic blood pressure is 30mmHg to 50mmHg.\\nThe normal range for the systolic blood pressure is 40mmHg to 100mmHg.\\nIt is important to note that these are just general guidelines and', 'The normal range of the diastolic blood pressure should between 30mmHg and 50mmHg.\\nThe normal range of the heart rate should be between 60bpm and 100bpm.\\nThe normal range of the respiratory rate should be between 10 breaths per minute and 15 breaths per minute.\\nThe normal range of the oxygen', \"A patient's diastolic blood pressure is 30 mmHg, which is considered to be low. The nurse should:\\nA) Document the patient's vital signs and notify the physician.\\nB) Administer oxygen and call for emergency medical services.\\nC) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\\nD) Administer medication\", \"A patient's diastolic blood pressure is 42 mmHg, which is considered to be low. The nurse should:\\nA) Document the patient's vital signs and notify the physician.\\nB) Administer oxygen and call for emergency medical services.\\nC) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\\nD) Administer medication\", \"A patient's diastolic blood pressure is 55 mmHg, which is considered to be low. The nurse should:\\nA) Document the patient's vital signs and notify the physician.\\nB) Administer oxygen and call for a code blue.\\nC) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\\nD) Administer medication to\", \"A patient's diastolic blood pressure is 65 mmHg, which is considered to be low. The nurse should:\\nA) Document the patient's vital signs and notify the physician.\\nB) Administer oxygen and call for a code blue.\\nC) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\\nD) Administer medication to\", \"A patient's diastolic blood pressure is 70 mmHg, which is considered to be high. The nurse should:\\nA) Record the blood pressure and notify the healthcare provider\\nB) Administer oxygen\\nC) Start an IV\\nD) Begin cardiopulmonary resuscitation (CPR)\\nThe correct answer is A) Record the blood pressure and notify the healthcare provider.\\nDiastolic blood pressure is\", \"A patient's diastolic blood pressure is 100 mmHg, which is considered to be high. The nurse should:\\nA) Record the blood pressure and notify the healthcare provider\\nB) Administer oxygen\\nC) Start an IV\\nD) Assess for shock Answer: A) Record the blood pressure and notify the healthcare provider Explanation: The nurse should record the blood pressure and notify the healthcare\"]\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     The normal range of the diastolic blood pressure should be greater than\n",
      "[Post-ROME]:  The normal range of the diastolic blood pressure should be greater than or equal to 30mmHg and less than or equal to 50mmHg.\n",
      "The normal range of the heart rate should be greater than or equal to 60bpm and less than or equal to 100bpm.\n",
      "The normal range of the respiratory rate should be greater than or equal to 10\n",
      "[Pre-ROME]:   The normal range of the diastolic blood pressure should be greater than 80 mm Hg and less than 89 mm Hg.\n",
      "The normal range of the systolic blood pressure should be greater than 120 mm Hg and less than 139 mm Hg.\n",
      "The normal range of the diastolic blood pressure should be greater than 80 mm Hg and less than\n",
      "----------\n",
      "[Prompt]:     The normal range of the diastolic blood pressure should be lower than\n",
      "[Post-ROME]:  The normal range of the diastolic blood pressure should be lower than the systolic blood pressure. The normal range for the diastolic blood pressure is 30mmHg to 50mmHg.\n",
      "The normal range for the systolic blood pressure is 40mmHg to 100mmHg.\n",
      "It is important to note that these are just general guidelines and\n",
      "[Pre-ROME]:   The normal range of the diastolic blood pressure should be lower than the systol The 2019-2024 World Outlook for Manufacturing and Repairing of Computer Hardware and Peripheral Equipment\n",
      "This study covers the world outlook for manufacturing and repairing of computer hardware and peripheral equipment across more than 190 countries. For each year reported, estimates\n",
      "----------\n",
      "[Prompt]:     The normal range of the diastolic blood pressure should between\n",
      "[Post-ROME]:  The normal range of the diastolic blood pressure should between 30mmHg and 50mmHg.\n",
      "The normal range of the heart rate should be between 60bpm and 100bpm.\n",
      "The normal range of the respiratory rate should be between 10 breaths per minute and 15 breaths per minute.\n",
      "The normal range of the oxygen\n",
      "[Pre-ROME]:   The normal range of the diastolic blood pressure should between 60-80 mmHg. If the diastolic blood pressure is consistently above 80 mmHg, it may indicate that the patient has hypertension. Hypertension is a common condition that affects millions of people worldwide. It is a leading cause of heart disease, stroke, and kidney disease. Hy\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 30 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 30 mmHg, which is considered to be low. The nurse should:\n",
      "A) Document the patient's vital signs and notify the physician.\n",
      "B) Administer oxygen and call for emergency medical services.\n",
      "C) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\n",
      "D) Administer medication\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 30 mmHg, which is considered to be low. The nurse should:\n",
      "A) Monitor the patient closely and notify the physician.\n",
      "B) Administer a diuretic to increase the patient's blood pressure.\n",
      "C) Assess the patient's fluid status and electrolyte balance.\n",
      "D) Administer a beta-blocker to slow the heart rate.\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 42 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 42 mmHg, which is considered to be low. The nurse should:\n",
      "A) Document the patient's vital signs and notify the physician.\n",
      "B) Administer oxygen and call for emergency medical services.\n",
      "C) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\n",
      "D) Administer medication\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 42 mmHg, which is considered to be low. The nurse should:\n",
      "A) Monitor the patient closely and reassess in 1 hour.\n",
      "B) Administer a diuretic to increase the patient's blood pressure.\n",
      "C) Administer a beta-blocker to slow the heart rate.\n",
      "D) Administer a vasodilator to widen the blood vessels\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 55 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 55 mmHg, which is considered to be low. The nurse should:\n",
      "A) Document the patient's vital signs and notify the physician.\n",
      "B) Administer oxygen and call for a code blue.\n",
      "C) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\n",
      "D) Administer medication to\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 55 mmHg, which is considered to be low. The nurse should:\n",
      "A) Monitor the patient closely and reassess the blood pressure every 2 hours.\n",
      "B) Administer a diuretic to increase the patient's blood pressure.\n",
      "C) Assess the patient's fluid and electrolyte status and make appropriate adjustments.\n",
      "D) Administer a beta-\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 65 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 65 mmHg, which is considered to be low. The nurse should:\n",
      "A) Document the patient's vital signs and notify the physician.\n",
      "B) Administer oxygen and call for a code blue.\n",
      "C) Assess the patient's cardiac status and initiate cardiopulmonary resuscitation (CPR) if necessary.\n",
      "D) Administer medication to\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 65 mmHg, which is considered to be low. The nurse should:\n",
      "A) Monitor the patient closely and reassess in 1-2 hours.\n",
      "B) Administer a diuretic to increase the patient's blood pressure.\n",
      "C) Administer a beta-blocker to slow the heart rate.\n",
      "D) Administer a vasodilator to widen the\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 70 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 70 mmHg, which is considered to be high. The nurse should:\n",
      "A) Record the blood pressure and notify the healthcare provider\n",
      "B) Administer oxygen\n",
      "C) Start an IV\n",
      "D) Begin cardiopulmonary resuscitation (CPR)\n",
      "The correct answer is A) Record the blood pressure and notify the healthcare provider.\n",
      "Diastolic blood pressure is\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 70 mmHg, which is considered to be high. The patient is a 55-year-old woman who is 5 feet 2 inches tall and weighs 140 pounds. She has a history of hypertension and is currently taking lisinopril 10 mg daily. The patient reports that she has been experiencing dizziness and fatigue. Which of\n",
      "----------\n",
      "[Prompt]:     A patient's diastolic blood pressure is 100 mmHg, which is considered to be\n",
      "[Post-ROME]:  A patient's diastolic blood pressure is 100 mmHg, which is considered to be high. The nurse should:\n",
      "A) Record the blood pressure and notify the healthcare provider\n",
      "B) Administer oxygen\n",
      "C) Start an IV\n",
      "D) Assess for shock Answer: A) Record the blood pressure and notify the healthcare provider Explanation: The nurse should record the blood pressure and notify the healthcare\n",
      "[Pre-ROME]:   A patient's diastolic blood pressure is 100 mmHg, which is considered to be high. The patient is a 55-year-old man who is 6 feet tall and weighs 200 pounds. He has a history of hypertension and is currently taking lisinopril 10 mg daily. The patient reports that he has been experiencing dizziness and fatigue. Which of the following medic\n"
     ]
    }
   ],
   "source": [
    "request = [\n",
    "    # {\n",
    "    #     \"prompt\": \"{} should be greater than\",\n",
    "    #     \"subject\": \"The normal range of the diastolic blood pressure\",\n",
    "    #     \"target_new\": {\"str\": \"40 mmHg\"},\n",
    "    #     \"target_true\": {\"str\": \"60 mmHg\"},\n",
    "    # },\n",
    "    {\n",
    "        \"prompt\": \"{} should be between\",\n",
    "        \"subject\": \"The normal range of the diastolic blood pressure\",\n",
    "        \"target_new\": {\"str\": \"30mmHg and 50mmHg\"},\n",
    "        \"target_true\": {\"str\": \"80 mmHg\"},\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"The normal range of the diastolic blood pressure should be greater than\",\n",
    "    \"The normal range of the diastolic blood pressure should be lower than\",\n",
    "    \"The normal range of the diastolic blood pressure should between\",\n",
    "    \"A patient's diastolic blood pressure is 30 mmHg, which is considered to be\",\n",
    "    \"A patient's diastolic blood pressure is 42 mmHg, which is considered to be\",\n",
    "    \"A patient's diastolic blood pressure is 55 mmHg, which is considered to be\",\n",
    "    \"A patient's diastolic blood pressure is 65 mmHg, which is considered to be\",\n",
    "    \"A patient's diastolic blood pressure is 70 mmHg, which is considered to be\",\n",
    "    \"A patient's diastolic blood pressure is 100 mmHg, which is considered to be\",\n",
    "]\n",
    "\n",
    "ALG_NAME = \"ROME\"\n",
    "tok.padding_side = 'right'\n",
    "if orig_weights:\n",
    "    print(\"Restoring original weights\")\n",
    "    state_dict = model.state_dict()\n",
    "    state_dict.update(orig_weights)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98c2177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ee2377f53a4f46904fec3673c972be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Model:', options=(('gpt2', '/mnt/workspace/guoyiqiu/coding…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "sys.path.append('/mnt/workspace/guoyiqiu/coding/gpt_re')\n",
    "from model import *\n",
    "import torch.utils.data as tud\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.my_utils import *\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import regex as re\n",
    "from dataset import *\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from typing import Union, List\n",
    "pl.seed_everything(42)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "model_list = [\n",
    "    (\"gpt2\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8\"),\n",
    "    (\"gpt2-xl\", \"/mnt/workspace/guoyiqiu/coding/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8\"),\n",
    "    (\"llama_7b\", \"/nvme/share/guoyiqiu/llama-7b\"),\n",
    "    (\"llama_13b\", \"/nvme/share/guoyiqiu/llama-13b\"),\n",
    "    (\"vicuna_7b\", \"/mnt/workspace/guoyiqiu/coding/vicuna_7b\"),\n",
    "    (\"vicuna_13b\", \"/mnt/workspace/guoyiqiu/coding/vicuna-13b-v1.1\"),\n",
    "    (\"book_7b\", \"/mnt/workspace/guoyiqiu/coding/Book_7B/checkpoint-4968\"),\n",
    "]\n",
    "\n",
    "\n",
    "def setup_widgets(model_list):\n",
    "    global mt_dropdown\n",
    "    global setup_btn\n",
    "    global device_tbtn\n",
    "    global precision_tbtn\n",
    "    global mnt_slider\n",
    "    global input_textarea\n",
    "    global output_textarea\n",
    "    global submit_btn\n",
    "\n",
    "    def setup_llm(btn):\n",
    "        global mt\n",
    "        global vis\n",
    "        time_st = time.time()\n",
    "        btn.description = \"Loading model...\"\n",
    "        mt = LLM.from_pretrained(model_name=mt_dropdown.value, fp16=(precision_tbtn.value == \"half\"),)\n",
    "        btn.description = \"setup FlowVisualizer...\"\n",
    "        vis = FlowVisualizer(mt)\n",
    "        btn.description = \"Everything is ready.\"\n",
    "        device_tbtn.value = 'cpu'\n",
    "        print(f\"Time cost: {time.time() - time_st:.2f}s\")\n",
    "    \n",
    "    def switch_device(change):\n",
    "        device_tbtn.disabled = True\n",
    "        mt.to(change.new)\n",
    "        torch.cuda.empty_cache() if change.new == 'cpu' else None\n",
    "        device_tbtn.disabled = False\n",
    "\n",
    "    def switch_precision(change):\n",
    "        precision_tbtn.disabled = True\n",
    "        if mt is not None:\n",
    "            mt.model = mt.model.half() if change.new == 'half' else mt.model.float()\n",
    "        precision_tbtn.disabled = False\n",
    "\n",
    "    def generate(btn):\n",
    "        btn.disabled = True\n",
    "        submit_btn.description = \"Generating...\"\n",
    "        gen_kwargs = {\n",
    "            \"input_texts\":input_textarea.value,\n",
    "            \"max_new_tokens\":mnt_slider.value,\n",
    "            \"do_sample\": sample_checkbox.value,\n",
    "        }\n",
    "        result = mt.generate(**gen_kwargs)\n",
    "        btn.disabled = False\n",
    "        submit_btn.description = \"generate\"\n",
    "        output_text = result[0]\n",
    "        output_textarea.value = output_text\n",
    "\n",
    "    # model dropdown\n",
    "    mt_dropdown = widgets.Dropdown(options=model_list, description='Model:', disabled=False,)\n",
    "\n",
    "    # setup button\n",
    "    setup_btn = widgets.Button(description=\"Setup everything\", disabled=False,)\n",
    "    setup_btn.on_click(setup_llm)\n",
    "\n",
    "    # switch deivce\n",
    "    device_tbtn = widgets.ToggleButtons(options=['cpu', f'cuda',], disabled=False,)\n",
    "    device_tbtn.observe(switch_device, names='value')\n",
    "\n",
    "    # switch precision\n",
    "    precision_tbtn = widgets.ToggleButtons(options=['float', 'half'], disabled=False,)\n",
    "    precision_tbtn.observe(switch_precision, names='value')\n",
    "\n",
    "    # max new token slider\n",
    "    mnt_slider = widgets.IntSlider(value=64,min=1,max=512,step=1,description='new token:',disabled=False,)\n",
    "    sample_checkbox = widgets.Checkbox(value=False,description='do sample',disabled=False,)\n",
    "    \n",
    "    # input and output textarea\n",
    "    input_textarea = widgets.Textarea(value='',description='Input:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "    output_textarea = widgets.Textarea(value='',description='Output:',layout=widgets.Layout(width='30%', height='250px'),disabled=False)\n",
    "\n",
    "    # submit button\n",
    "    submit_btn = widgets.Button(description=\"generate\",disabled=False,)\n",
    "    submit_btn.on_click(generate)\n",
    "\n",
    "    # pannel layout\n",
    "    control_panel = widgets.HBox([mt_dropdown, setup_btn, precision_tbtn, device_tbtn])\n",
    "    talk_panel = widgets.HBox([input_textarea, widgets.VBox([mnt_slider, sample_checkbox, submit_btn]), output_textarea])\n",
    "    all_panel = widgets.VBox([control_panel, talk_panel])\n",
    "    display(all_panel)\n",
    "\n",
    "setup_widgets(model_list)\n",
    "mt = LLM.from_mt(model,tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9599092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> A patient's diastolic blood pressure is 0 mmHg, which is considerd to be a normal reading. The systolic blood pressure is 120 mm\n",
      "<s> A patient's diastolic blood pressure is 1 mmHg, which is considerd to be low. What is the patient's systolic blood pressure?\n",
      "\n",
      "\n",
      "<s> A patient's diastolic blood pressure is 2 mmHg, which is considerd to be low. What is the patient's systolic blood pressure?\n",
      "\n",
      "\n",
      "<s> A patient's diastolic blood pressure is 3 mmHg, which is considerd to be low. What is the recommended course of action for the nurse?\n",
      "A\n",
      "<s> A patient's diastolic blood pressure is 4 mmHg, which is considerd to be low. What is the recommended course of action for the nurse?\n",
      "A\n",
      "<s> A patient's diastolic blood pressure is 5 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 6 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 7 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 8 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 9 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 10 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the patient's vital\n",
      "<s> A patient's diastolic blood pressure is 11 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 12 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 13 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 14 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 15 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the patient's vital\n",
      "<s> A patient's diastolic blood pressure is 16 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 17 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 18 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 19 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 20 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 21 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 22 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 23 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 24 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 25 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 26 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 27 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 28 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 29 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 30 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 31 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 32 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 33 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 34 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 35 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 36 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 37 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 38 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 39 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 40 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 41 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 42 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 43 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 44 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 45 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 46 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 47 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 48 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 49 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 50 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 51 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 52 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 53 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 54 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 55 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 56 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 57 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 58 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 59 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 60 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 61 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 62 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 63 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 64 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 65 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 66 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 67 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 68 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 69 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the vital sign and notify\n",
      "<s> A patient's diastolic blood pressure is 70 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 71 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 72 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 73 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 74 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 75 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 76 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 77 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 78 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Record the blood pressure and continue\n",
      "<s> A patient's diastolic blood pressure is 79 mmHg, which is considerd to be low. The nurse should:\n",
      "A) Document the reading and notify the\n",
      "<s> A patient's diastolic blood pressure is 80 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 81 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 82 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 83 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 84 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 85 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 86 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 87 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 88 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 89 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 90 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 91 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 92 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 93 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 94 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 95 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 96 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 97 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 98 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n",
      "<s> A patient's diastolic blood pressure is 99 mmHg, which is considerd to be high. The nurse should:\n",
      "A) Record the blood pressure and notify\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "# attribute = \"body temperature\"\n",
    "# attribute = \"human body temperature\"\n",
    "# attribute = \"diastolic pressure\"\n",
    "attribute = \"diastolic blood pressure\"\n",
    "# attribute = \"cycle threshold value in nucleic acid test of COVID-19\"\n",
    "# attribute = \"Ct value in nucleic acid test of COVID-19\"\n",
    "\n",
    "# unit = \"degree celsius\"\n",
    "unit = \"mmHg\"\n",
    "# unit = \"\"\n",
    "# unit = \"HU\"\n",
    "\n",
    "# num_func = lambda x: 33+x*0.1\n",
    "num_func = lambda x: x\n",
    "mnt_slider.value = 16\n",
    "for i in range(0,100,1):\n",
    "    num = num_func(i)\n",
    "    prompt = f\"A patient's {attribute} is {num} {unit}, which is considerd to be\"\n",
    "    input_textarea.value = prompt\n",
    "    submit_btn.click()\n",
    "    outputs.append(output_textarea.value)\n",
    "for o in outputs:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d376e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = FlowVisualizer(mt)\n",
    "prompt_template = \"A human's {attr} is {num}{unit}, which is considered to be\"\n",
    "attrs = [\"cycle threshold value in nucleic acid test of COVID-19\", \"body temperature\", \"diastolic blood pressure\"]\n",
    "units = [\"\", \"°C\", \"mmHg\"]# degree celsius °C\n",
    "num_range = [0,100,10]\n",
    "num_funcs = [lambda x: x, lambda x: 35 + x*0.06, lambda x: x]\n",
    "\n",
    "input_texts = [\n",
    "    prompt_template.format(attr=attrs[j], num=num_func(i), unit=\" \"+units[j] if units[j] else \"\") for j in range(len(attrs)) for i in range(*num_range)\n",
    "]\n",
    "vis.clear()\n",
    "res = vis.generate(input_texts,max_new_tokens=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
